{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/elkhyo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "dense_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def process_and_rank_sentences(context, question, min_length=50, method=\"dense\", top_k=5):\n",
    "    sentences = nltk.sent_tokenize(context)\n",
    "    merged_sentences = []\n",
    "    original_indices = []  # Track original indices\n",
    "    current_sentence = \"\"\n",
    "    current_index = 0  # Keep track of the current sentence index\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if len(current_sentence) + len(sentence) < min_length:\n",
    "            current_sentence += \" \" + sentence\n",
    "        else:\n",
    "            if current_sentence:\n",
    "                merged_sentences.append(current_sentence.strip())\n",
    "                original_indices.append(current_index)  # Record the starting index for this merged sentence\n",
    "            current_sentence = sentence\n",
    "            current_index = i\n",
    "    if current_sentence:\n",
    "        merged_sentences.append(current_sentence.strip())\n",
    "        original_indices.append(current_index)  # Record the last sentence's index\n",
    "    \n",
    "    if method == \"dense\":\n",
    "        question_embedding = dense_model.encode(question, convert_to_tensor=True)\n",
    "        sentence_embeddings = dense_model.encode(merged_sentences, convert_to_tensor=True)\n",
    "        \n",
    "        scores = util.cos_sim(question_embedding, sentence_embeddings)[0]\n",
    "        ranked_sentences = sorted(\n",
    "            zip(merged_sentences, scores.tolist(), original_indices), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        \n",
    "    elif method == \"bm25\":\n",
    "        tokenized_sentences = [nltk.word_tokenize(sent) for sent in merged_sentences]\n",
    "        bm25 = BM25Okapi(tokenized_sentences)\n",
    "        \n",
    "        scores = bm25.get_scores(nltk.word_tokenize(question))\n",
    "        ranked_sentences = sorted(\n",
    "            zip(merged_sentences, scores, original_indices), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "\n",
    "    top_k_sentences = sorted(ranked_sentences[:top_k], key=lambda x: x[2])\n",
    "\n",
    "    merged_text = ' '.join([sentence for sentence, _, _ in top_k_sentences])\n",
    "    \n",
    "    return merged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "test_dataset = load_dataset(\"theatticusproject/cuad-qa\",split='test')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def populate_generations(test_dataset, sample, generate_method, retrieval, top_k, device):\n",
    "    generated_ans = []\n",
    "    context_used = []\n",
    "\n",
    "    for id in tqdm(range(sample)):\n",
    "        full_context = test_dataset[id]['context']\n",
    "        question = test_dataset[id]['question']\n",
    "\n",
    "        if retrieval is not None:\n",
    "            context = process_and_rank_sentences(full_context, question, min_length=50, method=retrieval, top_k=top_k)\n",
    "            # top_k_sentences = ranked_sentences[:top_k]\n",
    "            # context = \" \".join(sentence for sentence, _ in top_k_sentences)\n",
    "            \n",
    "        # gold_answer = \" \".join(test_dataset[id]['answers']['text'])\n",
    "        # if gold_answer=='':\n",
    "        #     gold_answer = 'The answer is not present in the contract.'\n",
    "        else:\n",
    "            context = full_context\n",
    "        \n",
    "        max_new_tokens = 50\n",
    "\n",
    "        # ans_context =  model_generate(context, question, max_new_tokens, generate_method, device, full_context = full_context)\n",
    "\n",
    "        # print(f'Generated Answer: {ans_context}')\n",
    "        # print(f'Answer Gold: {gold_answer}')\n",
    "        # generated_ans.append(ans_context)\n",
    "        context_used.append(context)\n",
    "    return generated_ans, context_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hit_rate(context, original_answers):\n",
    "    hits = sum(1 for answer in original_answers if answer in context)\n",
    "    hit_rate_score = hits / len(original_answers) if len(original_answers)!=0 else np.NaN\n",
    "    return hit_rate_score\n",
    "\n",
    "def score_function(test_dataset, generated_ans, input_contexts):\n",
    "    #input_contexts = [x['context'] for x in test_dataset]\n",
    "    gold_answers_list = [x['answers']['text'] for x in test_dataset]\n",
    "    gold_answers = [\" \".join(x['answers']['text']) for x in test_dataset]\n",
    "    hit_rate_scores = [hit_rate(context, answer_list) for context, answer_list in zip(input_contexts, gold_answers_list)] \n",
    "\n",
    "    # gold_answers = ['Not present in the contract.' if answer == '' else answer for answer in gold_answers]\n",
    "    # generated_ans = ['None' if answer == '' else answer for answer in generated_ans]\n",
    "\n",
    "    # correct_scores =  alignscore_scorer(generated_ans, gold_answers)\n",
    "\n",
    "    # faith_gen_ans, faith_context = zip(*[(gen, context) for gen, context in zip(generated_ans, input_contexts) if gen != 'None' and gen != 'Not present in the contract.</s>'])\n",
    "\n",
    "    # faith_scores =  alignscore_scorer(faith_context, faith_gen_ans)\n",
    "\n",
    "    # faith_scores_full = [np.nan] * len(generated_ans)\n",
    "    # faith_index = 0\n",
    "\n",
    "    # for i, gen in enumerate(generated_ans):\n",
    "    #     if gen != 'None' and gen != 'Not present in the contract.</s>':\n",
    "    #         faith_scores_full[i] = faith_scores[faith_index]\n",
    "    #         faith_index += 1\n",
    "            \n",
    "    return None, None, hit_rate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4182/4182 [04:10<00:00, 16.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "test_dataset = load_dataset(\"theatticusproject/cuad-qa\",split='test')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "generate_method = 'Extend_Adaknn'\n",
    "retrieval= 'dense'\n",
    "top_k = 20\n",
    "\n",
    "generated_answers, used_context = populate_generations(test_dataset, len(test_dataset), generate_method, retrieval, top_k, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, nan, nan, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 1.0, nan, nan, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, 1.0, 0.4, 1.0, 1.0, 1.0, nan, nan, 1.0, nan, 0.0, 0.0, nan, 0.5, 1.0, nan, 1.0, nan, 1.0, 0.5, 0.0, nan, 0.4, nan, 0.3333333333333333, nan, 0.6666666666666666, 1.0, nan, 1.0, nan, nan, nan, nan, 1.0, 1.0, 0.5, 1.0, nan, 1.0, 0.5, nan, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, nan, 1.0, 0.0, nan, nan, nan, nan, 0.3333333333333333, nan, 0.2, 1.0, 0.0, nan, nan, nan, 0.0, 1.0, 0.125, 0.0, nan, nan, nan, 0.0, nan, 0.16666666666666666, 0.3333333333333333, 0.0, 0.5, nan, nan, 1.0, nan, nan, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.0, 0.0, 0.0, nan, nan, nan, 1.0, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, 0.0, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.42857142857142855, 1.0, nan, 0.0, 0.5, 0.0, 0.5, 1.0, nan, nan, 1.0, nan, 1.0, 1.0, nan, nan, nan, nan, 1.0, nan, nan, 1.0, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, nan, nan, nan, 0.0, 0.5, 1.0, 1.0, 1.0, nan, nan, 1.0, nan, 0.0, nan, nan, 0.5, nan, nan, 1.0, nan, nan, 1.0, 0.0, nan, 1.0, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, 1.0, nan, nan, nan, nan, nan, 1.0, 0.0, 1.0, 1.0, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, 1.0, 0.25, 0.0, nan, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.4, 0.0, 0.5, 1.0, nan, nan, 1.0, nan, 1.0, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.5, 0.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, nan, nan, 0.0, nan, nan, 0.4, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, 1.0, 1.0, nan, nan, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, 0.5, 1.0, nan, nan, nan, nan, nan, 1.0, nan, 1.0, 0.8, nan, nan, nan, nan, nan, 0.0, 0.5, 0.0, 0.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, nan, nan, nan, 1.0, nan, nan, 0.0, 0.5, 0.0, 0.0, 1.0, nan, nan, 1.0, nan, nan, 0.0, nan, 0.0, nan, nan, nan, 0.4, nan, 1.0, 0.0, nan, 0.0, nan, nan, 1.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, 0.0, 1.0, 1.0, nan, nan, nan, nan, nan, 0.0, 0.5, 1.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, nan, nan, nan, nan, nan, 1.0, nan, 1.0, 1.0, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, 1.0, nan, 0.0, 0.2, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, nan, nan, 0.0, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 0.0, nan, nan, nan, nan, 1.0, nan, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, nan, nan, 1.0, nan, 1.0, 0.0, nan, nan, nan, nan, 1.0, 0.1, 0.5, 0.3333333333333333, 0.0, nan, nan, nan, nan, 0.6, 0.0, 0.0, 0.0, 1.0, nan, 1.0, nan, 0.5, 1.0, 1.0, 1.0, nan, nan, 0.6, 0.0, nan, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.5, 0.0, 0.0, 1.0, nan, nan, 1.0, 1.0, 0.8, 0.0, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, 0.0, nan, nan, 0.6666666666666666, 0.0, nan, nan, nan, nan, nan, 0.5, 1.0, nan, 0.5555555555555556, nan, 1.0, nan, 0.0, nan, 0.0, 0.4, 0.0, 0.0, 0.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5, nan, nan, 0.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.0, 0.0, 0.3333333333333333, nan, nan, 1.0, 0.6, 1.0, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.0, 1.0, nan, 1.0, 1.0, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.5, 1.0, nan, 1.0, 1.0, 1.0, 1.0, nan, nan, 0.0, 0.0, nan, nan, nan, 1.0, 1.0, nan, 1.0, 0.0, nan, 0.0, nan, 1.0, 1.0, 0.0, nan, nan, nan, nan, nan, nan, 0.0, 0.2, nan, nan, 1.0, 1.0, 0.2857142857142857, 0.0, nan, 0.0, 0.25, 1.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, 0.0, nan, 0.0, nan, nan, nan, nan, nan, 1.0, 0.5, nan, 0.6666666666666666, 0.0, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, nan, nan, nan, 1.0, 1.0, nan, 0.0, 0.0, 0.0, nan, 1.0, 1.0, nan, 1.0, 0.0, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, 1.0, 0.6, 1.0, 1.0, nan, nan, nan, 1.0, nan, 1.0, nan, nan, nan, nan, nan, nan, 0.3333333333333333, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, 0.5, nan, nan, 0.0, nan, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, 0.2, 0.5, nan, 1.0, 0.0, 0.0, nan, 0.0, nan, 1.0, 1.0, 1.0, nan, 1.0, 1.0, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, 0.0, 0.0, 0.0, 0.0, nan, nan, nan, 1.0, nan, nan, 0.0, nan, nan, nan, nan, nan, 0.5, 0.5, 1.0, nan, nan, nan, nan, 0.0, nan, 0.09090909090909091, nan, 0.0, 0.8333333333333334, 0.0, 1.0, nan, 0.0, nan, nan, nan, nan, nan, nan, 0.6666666666666666, nan, 1.0, 0.25, 0.0, nan, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, 1.0, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 0.0, 0.5, nan, nan, 0.8, nan, nan, 0.0, 0.4, 0.0, 0.0, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, 0.7142857142857143, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, 1.0, 1.0, 0.0, nan, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, nan, 1.0, 0.5, nan, 1.0, nan, nan, nan, 1.0, 0.5, 1.0, nan, nan, 0.3333333333333333, nan, 0.0, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, nan, 0.0, 0.6, 0.0, 0.5, 1.0, nan, nan, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, 1.0, 1.0, 1.0, nan, nan, nan, nan, 1.0, nan, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.4, 1.0, nan, 1.0, nan, nan, 1.0, nan, 1.0, 1.0, nan, 0.0, nan, 0.0, nan, nan, 1.0, 1.0, nan, nan, nan, 0.0, nan, nan, 1.0, 1.0, nan, nan, nan, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.5, 0.0, 1.0, 1.0, nan, nan, 1.0, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 0.0, nan, 0.0, nan, nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, 0.6666666666666666, nan, nan, nan, nan, 1.0, nan, nan, 0.0, 0.0, 0.0, 0.0, nan, nan, nan, 1.0, nan, 1.0, nan, nan, 1.0, 1.0, nan, nan, nan, nan, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.14285714285714285, 0.0, nan, 1.0, nan, nan, 1.0, nan, nan, 0.0, nan, 1.0, nan, 0.0, nan, nan, nan, 1.0, nan, nan, 1.0, nan, nan, nan, 0.0, 0.5, nan, 1.0, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, 1.0, 0.0, nan, 1.0, 1.0, 1.0, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, 1.0, nan, nan, nan, nan, 1.0, 1.0, 0.75, 0.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.4, 0.0, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, 0.0, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 0.0, nan, 0.0, nan, nan, 0.0, 0.2, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, nan, 0.0, 0.0, nan, 0.0, nan, nan, 1.0, 0.0, nan, 0.6666666666666666, 0.0, nan, nan, nan, nan, nan, 0.0, 0.0, nan, 1.0, nan, nan, nan, 0.5, 0.5, 1.0, 1.0, nan, nan, 0.5, nan, nan, 1.0, 1.0, 1.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, 1.0, 1.0, 1.0, nan, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.5, 1.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.6666666666666666, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, 1.0, nan, nan, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, 1.0, 0.0, nan, 0.0, nan, nan, nan, 0.6666666666666666, 1.0, nan, nan, nan, nan, nan, 1.0, nan, nan, 1.0, nan, nan, 1.0, 0.0, nan, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, nan, 1.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, 0.0, nan, nan, nan, 0.6, 0.5, nan, 0.0, 0.0, 1.0, nan, 0.2, nan, nan, nan, nan, nan, nan, 0.5714285714285714, nan, nan, nan, nan, nan, 1.0, nan, nan, 0.0, 0.11764705882352941, 0.0, 0.0, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.5, 0.0, nan, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, 1.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.375, 0.0, nan, 0.5, nan, nan, 1.0, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, nan, nan, 1.0, nan, 1.0, 0.0, 0.0, nan, 0.0, 1.0, 1.0, 1.0, 1.0, nan, 0.5, 0.0, nan, 1.0, nan, nan, nan, nan, nan, 1.0, 0.5555555555555556, nan, 0.0, 0.0, 0.0, nan, 0.5, 0.3333333333333333, nan, nan, nan, nan, nan, 0.5, 1.0, 1.0, 1.0, 0.0, nan, nan, 0.6666666666666666, nan, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, nan, 1.0, nan, nan, nan, 1.0, nan, 1.0, 1.0, 0.0, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, nan, 0.6, nan, 0.0, 1.0, nan, nan, 1.0, 0.5, 1.0, 0.5, 1.0, nan, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.75, nan, 1.0, nan, nan, nan, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.75, 0.0, 0.0, 1.0, 0.0, nan, 1.0, nan, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, nan, 0.0, 1.0, 0.25, 0.0, nan, 0.16666666666666666, 0.0, 0.25, nan, 0.5, nan, nan, nan, nan, nan, nan, 0.0, 0.6666666666666666, nan, 1.0, 1.0, nan, 0.8235294117647058, 1.0, 1.0, 1.0, 0.4, 0.0, nan, 1.0, nan, nan, 1.0, 1.0, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, 1.0, 1.0, nan, 0.5, 0.3333333333333333, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, 0.0, nan, 1.0, 1.0, nan, 1.0, 1.0, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, nan, 1.0, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, nan, 0.0, 0.25, nan, nan, nan, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, 0.0, nan, 1.0, nan, nan, nan, 1.0, nan, 1.0, 1.0, 1.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 0.5, nan, nan, nan, nan, 1.0, nan, 0.3333333333333333, 1.0, 0.0, nan, nan, nan, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, nan, 1.0, nan, 0.5, 1.0, nan, 1.0, nan, nan, nan, 1.0, 1.0, 0.6, 1.0, nan, 1.0, 0.0, 0.6666666666666666, nan, 0.0, nan, nan, nan, nan, nan, nan, 0.3333333333333333, 0.75, nan, 1.0, nan, nan, 0.8333333333333334, 0.0, nan, 0.0, 0.5, 0.0, 0.5, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, 1.0, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, nan, 1.0, 0.6666666666666666, nan, nan, nan, nan, nan, 1.0, 0.8, 1.0, 1.0, 1.0, nan, nan, 1.0, nan, 1.0, nan, 1.0, nan, 1.0, nan, 1.0, nan, nan, 1.0, nan, nan, nan, nan, 0.25, nan, 1.0, nan, nan, nan, nan, 1.0, nan, 0.0, nan, nan, 1.0, nan, nan, nan, nan, nan, 0.0, 0.5, 0.0, 0.0, 1.0, nan, nan, 1.0, nan, nan, 0.3333333333333333, nan, 1.0, nan, nan, nan, nan, 1.0, 1.0, 0.0, nan, nan, nan, nan, nan, 0.5, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, 1.0, nan, nan, nan, nan, nan, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, nan, nan, 0.75, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.45454545454545453, 1.0, nan, nan, 0.0, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5, nan, 0.4444444444444444, nan, nan, nan, nan, nan, 0.0, 0.5, 0.0, nan, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, 1.0, nan, 1.0, nan, nan, 1.0, 0.0, nan, nan, nan, 0.8, nan, 1.0, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, nan, nan, nan, nan, 0.0, nan, nan, nan, nan, 1.0, nan, nan, 1.0, 1.0, nan, nan, 0.0, nan, nan, 0.0, nan, nan, 1.0, 1.0, nan, nan, nan, 0.5, nan, nan, nan, nan, 1.0, nan, nan, 0.0, 0.3, nan, nan, 1.0, nan, nan, 1.0, nan, nan, nan, 0.0, nan, 1.0, nan, 1.0, nan, 0.0, 1.0, nan, nan, nan, nan, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, 0.3333333333333333, nan, nan, nan, 0.0, nan, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.4, 0.0, 0.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, 1.0, nan, nan, 1.0, nan, 0.0, nan, 0.5, 1.0, nan, nan, nan, 1.0, nan, 0.5714285714285714, nan, nan, nan, nan, 0.4, 1.0, nan, nan, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, 0.5, nan, 1.0, 0.0, nan, nan, 0.0, nan, nan, nan, 0.0, 0.0, nan, nan, nan, nan, 0.4, 0.3333333333333333, 0.5, nan, 0.25, 1.0, nan, 0.5, nan, nan, 0.0, 0.3, 0.0, nan, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.0, 0.0, 1.0, 1.0, nan, nan, 1.0, nan, 1.0, 0.0, nan, 0.3333333333333333, 1.0, 0.5, nan, 0.18181818181818182, 0.5714285714285714, 0.2, 0.0, nan, nan, nan, nan, nan, 0.42857142857142855, 0.375, nan, nan, nan, 1.0, nan, 0.0, 0.42857142857142855, nan, 0.0, 0.5, nan, 1.0, 0.0, nan, 0.0, 0.25, nan, 1.0, 1.0, nan, nan, 1.0, nan, 0.3333333333333333, 0.0, nan, 0.6666666666666666, nan, nan, 1.0, nan, 0.75, 0.3333333333333333, nan, nan, nan, nan, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 0.0, nan, 1.0, 1.0, nan, 1.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.0, 0.0, 0.0, 1.0, nan, nan, 1.0, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, 0.16666666666666666, 0.3333333333333333, 1.0, 0.5, 0.0, nan, nan, 0.0, nan, nan, nan, 1.0, 0.5, 1.0, 0.3333333333333333, nan, nan, nan, nan, nan, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, 1.0, 0.3333333333333333, nan, nan, nan, nan, 1.0, nan, 1.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, 0.0, 0.4, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, 1.0, nan, nan, nan, nan, nan, 0.0, 0.75, 0.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, 0.6666666666666666, 0.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, 0.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, 0.0, 1.0, 1.0, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.0, 0.0, 0.0, 0.0, nan, nan, 1.0, nan, 0.0, 0.0, nan, nan, nan, nan, nan, 0.0, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5, 0.0, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.5, 0.0, 0.0, 0.0, nan, nan, 1.0, nan, nan, 0.0, nan, nan, nan, nan, 1.0, nan, 1.0, 1.0, nan, nan, nan, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, nan, nan, nan, nan, nan, nan, 1.0, nan, 1.0, nan, nan, nan, 0.75, nan, 0.0, 0.5, 0.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, 0.0, 0.0, nan, 0.0, 1.0, 1.0, 0.0, 0.5, nan, 0.1, nan, 0.5, 0.0, 0.5, nan, nan, 0.0, 0.25, 0.3333333333333333, 0.0, nan, 0.16666666666666666, nan, 0.0, nan, 0.0, 1.0, 0.0, nan, nan, 0.0, nan, 0.0, 0.75, nan, 0.5, 1.0, nan, 0.375, 0.25, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, 0.5, nan, 0.3333333333333333, 0.3333333333333333, nan, nan, nan, nan, nan, 0.0, nan, nan, nan, nan, 0.5, nan, nan, nan, 1.0, 1.0, nan, nan, nan, nan, nan, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.2222222222222222, 0.0, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 1.0, nan, 1.0, nan, nan, nan, 1.0, 0.6, 0.0, 1.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 0.6, 0.0, 0.5, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, 1.0, 0.4, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, nan, nan, 1.0, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.4, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, nan, 1.0, nan, nan, nan, nan, nan, 0.0, nan, 0.5, 0.8333333333333334, 1.0, 0.6666666666666666, nan, 1.0, nan, 1.0, nan, nan, nan, nan, nan, nan, 0.0, nan, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 0.0, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, nan, 1.0, nan, nan, 0.0, nan, nan, nan, nan, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.75, nan, nan, 1.0, 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, 0.5, 0.0, 0.0, 1.0, nan, nan, 1.0, nan, nan, nan, nan, nan, 1.0, nan, 1.0, nan, nan, 1.0, nan, nan, nan, nan, 1.0, nan, 1.0, nan, nan, nan, nan, 1.0, nan, nan, nan, 1.0, 1.0, nan, nan, 1.0, nan, nan]\n",
      "Average Hit Rate Score: 0.6193571977168535\n"
     ]
    }
   ],
   "source": [
    "_, _, hit_rate_scores = score_function(test_dataset, generated_answers, used_context)\n",
    "print(hit_rate_scores)\n",
    "print(f'Average Hit Rate Score: {np.nanmean(hit_rate_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
