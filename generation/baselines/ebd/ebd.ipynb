{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Tuple\n",
    "from typing import Union, List, Literal, Optional\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from slack_notifier import send_slack_notification\n",
    "\n",
    "\n",
    "print(f\"Python Version : {sys.version}\")\n",
    "print(f\"Torch Version : {torch.__version__}\")\n",
    "print(f\"Transformers Version : {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Tuple\n",
    "from typing import Union, List, Optional\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EBD:\n",
    "    def __init__(self,\n",
    "                 model_name: str,\n",
    "                 device: Union[int,str] = 0):\n",
    "        device_map = torch.device(f\"cuda:{device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map, use_cache=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)\n",
    "        # self.model = torch.compile(self.model)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "        self.device = device_map\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "    def construct_context_based_inputs(self,\n",
    "                                       prompts: List[str],\n",
    "                                       context_prefix: str = None,\n",
    "                                       contexts: List[List[Tuple[Optional[str], str]]] = None) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        Construct input strings with contexts for the model\n",
    "        Args:\n",
    "            prompts: List of prompts to generate completions for\n",
    "            context_prefix: Prefix to add to the context before each prompt\n",
    "            contexts: List of lists of tuples containing context IDs and context texts\n",
    "        Returns:\n",
    "            List of input strings with contexts\n",
    "        \"\"\"\n",
    "        \n",
    "        inputs_with_contexts = []\n",
    "        inputs_with_contexts_to_prompt_index = []\n",
    "        for prompt_index, prompt in enumerate(prompts):\n",
    "            if contexts is not None:\n",
    "                context_list = contexts[prompt_index]\n",
    "                for context_id, context_text in context_list:\n",
    "                    if len(context_text) > 0:\n",
    "                        if context_id is not None:\n",
    "                            context_prefix = context_text.format(context_id)\n",
    "                        inputs_with_contexts.append(f\"{context_prefix}\\n{context_text} {self.tokenizer.eos_token} {prompt}\")\n",
    "                        inputs_with_contexts_to_prompt_index.append(prompt_index)\n",
    "            else:\n",
    "                inputs_with_contexts.append(prompt)\n",
    "                inputs_with_contexts_to_prompt_index.append(prompt_index)\n",
    "        return inputs_with_contexts\n",
    "        \n",
    "    def predict_next_token(self, \n",
    "                           logits: torch.Tensor, \n",
    "                           decoding_strategy: str, \n",
    "                           top_p: float, \n",
    "                           top_k: int, \n",
    "                           use_repetition_penalty: bool, \n",
    "                           repetition_penalty_value: float, \n",
    "                           generated_tokens: List[set] = None\n",
    "                           ) -> torch.Tensor :\n",
    "        # * Repetitin Penalty 참고 코드 : https://huggingface.co/transformers/v2.11.0/_modules/transformers/modeling_utils.html#PreTrainedModel.enforce_repetition_penalty_\n",
    "        if use_repetition_penalty:\n",
    "            assert repetition_penalty_value >= 1.0, \"Repetition penalty must be >= 1.\"\n",
    "            mask = torch.zeros_like(logits)\n",
    "            for i, token_set in enumerate(generated_tokens):\n",
    "                mask[i, list(token_set)] = 1.0\n",
    "            penalty = torch.where(mask == 1.0, repetition_penalty_value, 1.0) # generated_tokens에 있는 토큰들은 penalty를 repetition_penalty_value로, 없는 토큰들은 1.0(현상 유지)으로 설정\n",
    "            logits *= torch.where(logits < 0, penalty, 1.0/penalty) # if logit is smaller than 0, multiply with penalty, else divide by penalty\n",
    "        \n",
    "        if decoding_strategy == 'top_p':\n",
    "            assert top_p is not None, \"top_p must be provided for top_p sampling\"\n",
    "            logits = self._top_p_sampling(logits, top_p)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "\n",
    "        elif decoding_strategy == 'top_k':\n",
    "            assert top_k is not None, \"top_k must be provided for top_k sampling\"\n",
    "            logits = self._top_k_sampling(logits, top_k)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "\n",
    "        elif decoding_strategy == 'greedy':\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        return next_token\n",
    "    \n",
    "    def plot_next_word_distribution(self, logits, top_k=10):\n",
    "        \"\"\"\n",
    "        Plots the probability distribution of the next word from logits.\n",
    "\n",
    "        Parameters:\n",
    "        - logits: Tensor, the raw logits output by the model for the next word.\n",
    "        - top_k: int, the number of most probable words to display in the plot.\n",
    "        \"\"\"\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
    "        top_k_words = [self.tokenizer.decode([idx]) for idx in top_k_indices.tolist()[0]]\n",
    "\n",
    "        # Plot the distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(top_k_words, top_k_probs.tolist()[0], alpha=0.7)\n",
    "        plt.xlabel('Words')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.title('Next Word Probability Distribution')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def compute_maximum_entropy_layer_probs(self,\n",
    "                                            input_ids: torch.Tensor,\n",
    "                                            attention_mask: torch.Tensor,\n",
    "                                            temperature: float = 1.0) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids,\n",
    "                                 attention_mask=attention_mask,\n",
    "                                 return_dict=True,\n",
    "                                 output_hidden_states=True)\n",
    "        # batch_layer_entropies = []\n",
    "        # for hidden_state_l in outputs.hidden_states:\n",
    "        #     current_rep_hidden_states = hidden_state_l[:, -1:, :]  \n",
    "        #     current_rep_logits = torch.matmul(current_rep_hidden_states, self.model.lm_head.weight.t())  # (batch_size, vocab_size)\n",
    "        #     probs = F.softmax(current_rep_logits / temperature, dim=-1) \n",
    "        #     batch_layer_entropies.append(-torch.sum(probs * torch.log(probs), dim=-1))\n",
    "        # entropies_tensor = torch.stack(batch_layer_entropies)\n",
    "        \n",
    "        \n",
    "        entropies_tensor = torch.stack([\n",
    "            -torch.sum(F.softmax(torch.matmul(hidden_state_l[:, -1:, :], self.model.lm_head.weight.t()) / temperature, dim=-1) * \n",
    "            torch.log_softmax(torch.matmul(hidden_state_l[:, -1:, :], self.model.lm_head.weight.t()) / temperature, dim=-1), dim=-1)\n",
    "            for hidden_state_l in outputs.hidden_states\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        maximum_entropy_layer_indices = torch.max(entropies_tensor, dim=0).indices.squeeze(-1)\n",
    "        maximum_entropy_layer_probs = []\n",
    "        for prompt_index, maximum_entropy_layer_index in enumerate(maximum_entropy_layer_indices):\n",
    "            maximum_entropy_layer = outputs.hidden_states[maximum_entropy_layer_index][:, -1:, :]\n",
    "            maximum_entropy_layer_logits = torch.matmul(maximum_entropy_layer, self.model.lm_head.weight.t())\n",
    "            maximum_entropy_layer_probs.append(maximum_entropy_layer_logits[prompt_index])\n",
    "        return torch.stack(maximum_entropy_layer_probs).squeeze(1)\n",
    "    \n",
    "    def compute_le_ens_scores(self,\n",
    "                              input_ids_with_context: torch.Tensor,\n",
    "                              attention_mask_with_context: torch.Tensor,\n",
    "                              context_lengths: List[int],\n",
    "                              temperature: float = 1.0) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids_with_context,\n",
    "                                 attention_mask=attention_mask_with_context,\n",
    "                                 return_dict=True,\n",
    "                                 output_hidden_states=True)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        batched_logits_per_prompt = next_token_logits.split(context_lengths)\n",
    "        assert batched_logits_per_prompt[0].shape[0] == context_lengths[0]\n",
    "        \n",
    "        le_ens_scores = []\n",
    "        for logits in batched_logits_per_prompt:\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            le_ens_score_entropy = F.softmax(torch.sum(probs * torch.log(probs), dim=-1), dim=-1)\n",
    "            le_ens_scores.append(torch.sum(le_ens_score_entropy.unsqueeze(1) * torch.log(probs), dim=0))\n",
    "        return torch.stack(le_ens_scores)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def generate(self, \n",
    "                prompts: List[str],\n",
    "                context_prefix: str = None,\n",
    "                contexts: Optional[List[str]] = None, \n",
    "                beta: float = 0.5,\n",
    "                max_length: int = 256,\n",
    "                decoding_strategy: str = 'top_p',\n",
    "                top_p_value: float = 0.9,\n",
    "                top_k_value: int = 20,\n",
    "                use_repetition_penalty: bool = False, \n",
    "                repetition_penalty_value: float = 1.0,\n",
    "                ) -> List[List[int]]:\n",
    "\n",
    "        tokenized_inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.model.config.max_position_embeddings)\n",
    "        tokenized_inputs = {key: value.to(self.model.device) for key, value in tokenized_inputs.items()}\n",
    "        input_ids = tokenized_inputs['input_ids']\n",
    "        attention_mask = tokenized_inputs['attention_mask']\n",
    "        \n",
    "        \n",
    "        inputs_with_contexts = self.construct_context_based_inputs(prompts,\n",
    "                                                                   context_prefix,\n",
    "                                                                   contexts)\n",
    "        tokenized_inputs_with_contexts = self.tokenizer(inputs_with_contexts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.model.config.max_position_embeddings)\n",
    "        tokenized_inputs_with_contexts = {key: value.to(self.model.device) for key, value in tokenized_inputs_with_contexts.items()}\n",
    "        input_ids_with_context = tokenized_inputs_with_contexts['input_ids']\n",
    "        attention_mask_with_context = tokenized_inputs_with_contexts['attention_mask']\n",
    "        \n",
    "        cur_len = 0\n",
    "        batch_size = len(input_ids)\n",
    "        unfinished_sents = input_ids.new(batch_size).fill_(1)\n",
    "        sent_lengths = input_ids.new(batch_size).fill_(max_length)\n",
    "\n",
    "        generated_tokens = [[] for _ in range(batch_size)] # e.g., [[4132, 102, 29402], [2378, 7893, 23001]]\n",
    "        context_lengths = [len(context) for context in contexts]\n",
    "\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(total=max_length, desc=\"EBD'ing\", position=0)\n",
    "            while cur_len < max_length:\n",
    "                maximum_entropy_layer_probs = self.compute_maximum_entropy_layer_probs(input_ids,\n",
    "                                                                                       attention_mask)\n",
    "                le_ens_scores = self.compute_le_ens_scores(input_ids_with_context,\n",
    "                                                           attention_mask_with_context,\n",
    "                                                           context_lengths)\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "                # next_token_logits = outputs.logits[:, -1, :] \n",
    "                self.plot_next_word_distribution(outputs.logits[:, -1, :])\n",
    "                next_token_logits = (1 + beta) * le_ens_scores - beta * torch.log(maximum_entropy_layer_probs)\n",
    "                self.plot_next_word_distribution(next_token_logits)\n",
    "                raise ValueError(\"Stop\")\n",
    "                next_token = self.predict_next_token(logits=next_token_logits, \n",
    "                                                    decoding_strategy=decoding_strategy, \n",
    "                                                    top_p=top_p_value, \n",
    "                                                    top_k=top_k_value, \n",
    "                                                    use_repetition_penalty=use_repetition_penalty, \n",
    "                                                    repetition_penalty_value=repetition_penalty_value, \n",
    "                                                    generated_tokens=[set(tokens) for tokens in generated_tokens])\n",
    "\n",
    "                input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "                attention_mask = torch.cat([attention_mask, torch.ones((batch_size, 1), device=self.device)], dim=-1)\n",
    "                repeated_next_token = torch.cat([\n",
    "                    next_token[i].repeat(context_lengths[i]) for i in range(len(next_token))\n",
    "                ], dim=0)\n",
    "                input_ids_with_context = torch.cat([input_ids_with_context, repeated_next_token.unsqueeze(-1)], dim=-1)\n",
    "                attention_mask_with_context = torch.cat([attention_mask_with_context, torch.ones((input_ids_with_context.shape[0], 1), device=self.device)], dim=-1)\n",
    "                \n",
    "                cur_len += 1\n",
    "\n",
    "                # Update generated tokens and check for completion\n",
    "                for i, token in enumerate(next_token.tolist()):\n",
    "                    if unfinished_sents[i] == 1:\n",
    "                        generated_tokens[i].append(token)\n",
    "                    if unfinished_sents[i] == 1 and token == self.tokenizer.eos_token_id:\n",
    "                        unfinished_sents[i] = 0\n",
    "                        sent_lengths[i] = cur_len\n",
    "\n",
    "                # Check for sentences that are finished\n",
    "                if self.tokenizer.eos_token_id is not None:\n",
    "                    eos_in_sents = next_token == self.tokenizer.eos_token_id\n",
    "                    is_sents_unfinished_and_token_to_add_is_eos = unfinished_sents.mul(eos_in_sents.long()).bool()\n",
    "                    sent_lengths.masked_fill_(is_sents_unfinished_and_token_to_add_is_eos, cur_len)\n",
    "                    unfinished_sents.mul_((~eos_in_sents).long())\n",
    "\n",
    "                # Break if all sentences are finished : stop when there is a EOS token in each sentence, or if we exceed the maximul length\n",
    "                if unfinished_sents.max() == 0:\n",
    "                    break\n",
    "                pbar.update(1)\n",
    "        return generated_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(1002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebd_model = EBD(model_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "                device=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 : Compare w/ or w/o using Context-aware Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prefix = \"\"\"\n",
    "Here is a reference case, which you can use and must explicitly mention its reference id which is {reference_id}.\n",
    "\"\"\"\n",
    "\n",
    "contexts = [[\n",
    "(\n",
    "    \"114 F.3d 596\",\n",
    "    \"\"\"\n",
    "    Relations Act, 29 U.S.C. § 185. The parties filed cross-motions for summary judgment, and the district court enforced the award. The Beacon Journal filed this timely appeal. II. This court reviews the district court’s grant of summary judgment de novo. Rowley v. United States, 76 F.3d 796, 799 (6th Cir.1996). Nevertheless, our scope of review, like the review of the district court, is extremely limited. The Supreme Court has made clear in the Steelworkers’ Trilogy and its progeny that courts must accord an arbitrator’s decision substantial deference because it is the arbitrator’s construction of the agreement, not the court’s construction, to which the parties have agreed. See United Paperworkers Int’l Union v. Misco, 484 U.S. 29, 37-8, 108 S.Ct. 364, 371, 98 L.Ed.2d 286 (1987) (“Because the parties have contracted to have disputes settled by an arbitrator chosen by them rather than by a judge, it is the arbitrator’s view of the facts and of the meaning of the contract that they have agreed to accept.”). Hence, our review is extremely limited. We review the arbitrator’s decision only to determine whether the arbitrator was “arguably construing or applying the contract and acting within the scope of his authority.” Id. at 38, 108 S.Ct. at 371. If the arbitrator’s award “draws its essence from the collective bargaining agreement,” and is not merely the arbitrator’s “own brand of industrial justice,” the award is legitimate. United Steelworkers of Am. v. Enterprise Wheel & Car Co., 363 U.S. 593, 597, 80 S.Ct. 1358, 1361, 4 L.Ed.2d 1424 (1960). Courts will not weigh the merits of the claim or determine whether the claim is supported by language in the written instrument; otherwise, the policy of settling labor disputes through arbitration would be undermined. Misco, 484 U.S. at 36, 108 S.Ct. at 369-70; see also Unite\n",
    "    \"\"\"\n",
    "),\n",
    "(\n",
    "    \"114 F.3d 596\",\n",
    "    \"\"\"\n",
    "    any evidence that a member had “to modify or change his/her vacation plans due to the management’s ‘new interpretation of its rights under the vacation and management rights clauses of the labor agreement.” Arbitrator’s Decision, Slip op. at 6. In contrast, management was “vague on the specifics of not being able to meet the necessities of the supervisors and the production needs of the newspaper.” Id. The arbitrator made no further findings, but instead found that the Union’s grievance was justified. He then crafted his own solution, whereby the four new supervisors and the Union employees were thrown into a “seniority pool” for vacation selection purposes. He also provided for a grievance procedure through the Union for employees that believed they were adversely affected by the new procedure. The Beacon Journal refused to comply with the arbitration award and instead instituted this lawsuit under section 801 of the Labor Management Relations Act, 29 U.S.C. § 185. The parties filed cross-motions for summary judgment, and the district court enforced the award. The Beacon Journal filed this timely appeal. II. This court reviews the district court’s grant of summary judgment de novo. Rowley v. United States, 76 F.3d 796, 799 (6th Cir.1996). Nevertheless, our scope of review, like the review of the district court, is extremely limited. The Supreme Court has made clear in the Steelworkers’ Trilogy and its progeny that courts must accord an arbitrator’s decision substantial deference because it is the arbitrator’s construction of the agreement, not the court’s construction, to which the parties have agreed. See United Paperworkers Int’l Union v. Misco, 484 U.S. 29, 37-8, 108 S.Ct. 364, 371, 98 L.Ed.2d 286 (1987) (“Because the parties have contracted to have disputes settled by an arbitrator chosen by them rather than by a judge, it is the arbitrator’s view\n",
    "    \"\"\"\n",
    "),\n",
    "(\n",
    "    \"114 F.3d 596\",\n",
    "    \"\"\"\n",
    "    of the facts and of the meaning of the contract that they have agreed to accept.”). Hence, our review is extremely limited. We review the arbitrator’s decision only to determine whether the arbitrator was “arguably construing or applying the contract and acting within the scope of his authority.” Id. at 38, 108 S.Ct. at 371. If the arbitrator’s award “draws its essence from the collective bargaining agreement,” and is not merely the arbitrator’s “own brand of industrial justice,” the award is legitimate. United Steelworkers of Am. v. Enterprise Wheel & Car Co., 363 U.S. 593, 597, 80 S.Ct. 1358, 1361, 4 L.Ed.2d 1424 (1960). Courts will not weigh the merits of the claim or determine whether the claim is supported by language in the written instrument; otherwise, the policy of settling labor disputes through arbitration would be undermined. Misco, 484 U.S. at 36, 108 S.Ct. at 369-70; see also United Steelworkers of Am. v. American Mfg. Co., 363 U.S. 564, 568, 80 S.Ct. 1343, 1346, 4 L.Ed.2d 1403 (1960) (“[C]ourts, therefore, have no business weighing the merits of the grievance, considering whether there is equity in a particular claim, or determining whether there is particular language in the written instrument which will support the claim.”). Despite the great amount of deference accorded an arbitrator’s decision, our review is not toothless when an arbitrator’s award disregards the collective bargaining agreement and its terms. See Lattimer-Stevens Co. v. United Steelworkers, 913 F.2d 1166, 1171-72 (6th Cir.1990) (Boggs, J., dissenting) (delineating eases setting aside arbitrator’s decision). Even though arbitrators are not flawless, courts must refrain from reversing an arbitrator simply because the court disagrees with the result or believes the arbitrator made a serious legal or factual error. Misco, 484 U.S. at 38, 108 S.Ct. at 371 (“that a court is convinced [the\n",
    "    \"\"\"\n",
    ")\n",
    "]]\n",
    "\n",
    "prompts = [\"\"\"\n",
    "Continue to write the following case using the style of my write up. Your answer contains from 100 to 400 words. Make your answer concise, relevant and avoid redundant language.\n",
    "\n",
    "BEER, District Judge.\n",
    "Alken-Ziegler, Incorporated, (Company) appeals from the district court’s grant of summary judgment affirming an arbitration award in favor of the International Union, United Automobile, Aerospace and Agricultural Implement Workers of America, and Local Union 985 (Union). For the following reasons, we find that, even in light of our deferential review, the arbitrator disregarded the provisions of the labor contract. Therefore, we reverse the district court’s decision and vacate the arbitration award.\n",
    "I\n",
    "The Company and the Union were parties to a labor contract effective December 15, 1999. In March, 2001, the Company notified the Union that it would be closing its Novi plant and that it would be necessary to terminate all of the employees at the facility. As a result of the plant closing on October 17, 2001, all but one employee was terminated during the calendar year, 2001. The Company refused to pay vacationpay benefits to employees who did not work for the Company on January 1, 2002. The Union filed a grievance.\n",
    "Article 16 (61) of the labor agreement sets forth the eligibility requirement for payment of vacation benefits:\n",
    "(a) Employees shall be eligible for vacations, time off and vacation pay as set forth below.\n",
    "(b) For purposes of eligibility, the vacation year will be considered the calendar year period from January 1st to December 31.\n",
    "(c) An employee covered by the agreement who is actually working on January 1st of any year and who has at least six (6) months seniority and has' worked at least eight hundred (800) hours from and after January 1st of the previous year shall be paid the equivalent of two-and-one half (2-1/2) days vacation pay.\n",
    "ijs ifc tjc %\n",
    "(f) Employees with twelve (12) months or more of seniority who have worked more than eight hundred (800) hours, but less than sixteen hundred (1600) hours, during the vacation year, shall receive a pro-rated vacation pay on the basis of the ratio of their actual hours to sixteen hundred (1600) hours, but not to exceed the full vacation pay to which they were entitled by reason of their seniority and hours worked as set forth above.\n",
    "(g) Vacation pay will be computed on a straight time forty (40) hour basis including applicable shift premium. The employee’s hour basis including applicable shift premium. The employee’s hourly rate in effect when vacation is taken will be used to compute vacation pay. If an employee is laid off after six (6) months service, their vacation pay will be pro-rated same as above.\n",
    "Pursuant to Article 5 of the labor contract, the parties arbitrated the grievance. At the arbitration the Union asserted that because it was not the employees’ fault that they were unable to work the full year, the employees were entitled to their vacation pay. The arbitrator granted the grievance, allowing all plaintiffs, who, but for being laid off, would have been able to continue employment and thereby qualify for vacation benefits. The arbitrator reasoned that “[i]t would be unreasonable to cause such forfeitures particularly where an employee has no control over the situation.”\n",
    "The Company filed a complaint in the district court asserting that the arbitrator’s award contradicted the clear, mandatory commands of the labor contract, which required that an employee be “actually working” for the Company as of January 1, 2002, to receive vacation pay. The district court granted the Union’s motion for summary judgment and upheld the arbitrator’s award. The Company appealed.\n",
    "II\n",
    "\"\"\"]\n",
    "\n",
    "max_length = 4\n",
    "decoding_strategy = 'greedy'\n",
    "use_repetition_penalty = True\n",
    "repetition_penalty_value = 1.2\n",
    "method = 'ebd'\n",
    "\n",
    "outputs = ebd_model.generate(\n",
    "                            prompts=prompts,\n",
    "                            contexts=contexts,\n",
    "                            max_length=max_length,\n",
    "                            decoding_strategy=decoding_strategy,\n",
    "                            context_prefix=context_prefix,\n",
    "                            beta=0.5,\n",
    "                            use_repetition_penalty=use_repetition_penalty,\n",
    "                            repetition_penalty_value=repetition_penalty_value,\n",
    "                            )\n",
    "print(\"Answer:\")\n",
    "print(ebd_model.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
    "\n",
    "rep_extension = f'_rep_{f\"{use_repetition_penalty}_rep_value_{repetition_penalty_value}\" if use_repetition_penalty else use_repetition_penalty}'\n",
    "filename = f\"../basement/cad_generations/output_{method}_{decoding_strategy}_rep_{rep_extension}_{max_length}.txt\"\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'w') as file:\n",
    "    file.write(\"Prompt:\\n\")\n",
    "    file.write(prompts[0])\n",
    "    file.write(\"\\n\\nAnswer:\\n\")\n",
    "    file.write(ebd_model.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = ebd_model.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for i, output in enumerate(decoded_output):\n",
    "    print(f\"Output {i}: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
