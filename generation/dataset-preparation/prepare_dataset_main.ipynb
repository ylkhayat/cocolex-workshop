{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import argparse\n",
    "import bm25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 300\n",
    "chunk_overlap = 0.5\n",
    "\n",
    "num_proc = os.cpu_count() - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"echr\"\n",
    "\n",
    "if dataset == \"clerc\":\n",
    "    original_dataset = load_dataset(\"jhu-clsp/CLERC\", data_files={\"train\": f\"generation/train.jsonl\",  \"test\": f\"generation/test.jsonl\"})\n",
    "    workshop_hf_name = f\"CLERC-generation-workshop\"\n",
    "    key_field = \"docid\"\n",
    "elif dataset == \"echr\":\n",
    "    # original_dataset = load_dataset(\"echr\", \"eng\")\n",
    "    workshop_hf_name = f\"ECHR-generation-workshop\"\n",
    "    key_field = \"appno\"\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset\")\n",
    "current_chosen_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chunk_document_into_passages_overlap_split(document_id: str, text: str, max_len: int, overlap: float):\n",
    "#     if max_len <= 0:\n",
    "#         raise ValueError(\"max_len must be a positive integer\")\n",
    "#     if not (0 <= overlap < 1):\n",
    "#         raise ValueError(\"overlap must be between 0 (inclusive) and 1 (exclusive)\")\n",
    "\n",
    "#     words = text.split()\n",
    "#     chunks = []\n",
    "#     step = int(max_len * (1 - overlap))\n",
    "#     last_index = 0\n",
    "#     for index, i in enumerate(range(0, len(words), step)):\n",
    "#         current_splits = [document_id]\n",
    "#         if i == 0:\n",
    "#             second_split = words[i:i+max_len]\n",
    "#             last_index = i+max_len\n",
    "#             splits = [\"\", \" \".join(second_split)]\n",
    "#         else:\n",
    "#             first_split = words[last_index-step:last_index]\n",
    "#             second_split = words[last_index:last_index+step]\n",
    "#             last_index = last_index+step\n",
    "#             splits = [\" \".join(first_split), \" \".join(second_split)]\n",
    "#         if splits:\n",
    "#             current_splits.extend(splits)\n",
    "#             chunks.append(current_splits)\n",
    "#     return chunks\n",
    "\n",
    "# def chunk_citations_overlap_split(record):\n",
    "#     chunks = []\n",
    "#     for citation_id, citation in record['citations']:\n",
    "#         chunks.extend(chunk_document_into_passages_overlap_split(citation_id, citation, chunk_size, chunk_overlap))\n",
    "#     return chunks\n",
    "\n",
    "# if 'oracle_documents_passages_overlap_split' not in current_chosen_dataset.column_names['train'] or 'oracle_documents_passages_overlap_split' not in current_chosen_dataset.column_names['test']:\n",
    "#     print(f\"[*] adding oracle_documents_passages_overlap_split to {workshop_hf_name}\")\n",
    "#     if \"citations\" not in current_chosen_dataset.column_names['train'] or \"citations\" not in current_chosen_dataset.column_names['test']:\n",
    "#         current_chosen_dataset = DatasetDict({\n",
    "#             'train': current_chosen_dataset['train'].add_column(name=\"citations\", column=original_dataset['train']['citations']),\n",
    "#             'test': current_chosen_dataset['test'].add_column(name=\"citations\", column=original_dataset['test']['citations'])\n",
    "#         })\n",
    "#     current_chosen_dataset = current_chosen_dataset.map(lambda record: {'oracle_documents_passages_overlap_split': chunk_citations_overlap_split(record)})\n",
    "#     current_chosen_dataset = current_chosen_dataset.select_columns(['docid', 'previous_text', 'gold_text', 'citations' ,'oracle_documents_passages', 'oracle_documents_passages_overlap_split'])\n",
    "#     current_chosen_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\")\n",
    "# else:\n",
    "#     print(f\"[!] oracle_documents_passages_overlap_split already exists in {workshop_hf_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] adding oracle_documents_passages to ECHR-generation-workshop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485c47617f6b4d6abef3ae3c0a7b15dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e8c01123154f8a8684211ccf06d147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8599a3ad3e3411b8163a0739746ff1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a4b48ca64b4d9f854b5e214b8021eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613dbe299eb24a998d6f07dbea5ae5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662dd0b7e10d40e4b193831100d1d9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98099739aeee4408a109f1095a62e37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5498c08cddd4e5f8f932c8691d7d4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6e2936e0f24377a7438d735e451faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d385173e52c4f71bbac1c3a36b7dd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d854ed1b65e047a7bfed094b55e907d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce71eae2030b49d796887aebac7ad685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd03c9490b0b4b219b5ce866c451c8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/521 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ylkhayat/ECHR-generation-workshop/commit/a3b8b935570af329bda488250b54272ff082a76d', commit_message='Upload dataset', commit_description='', oid='a3b8b935570af329bda488250b54272ff082a76d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ylkhayat/ECHR-generation-workshop', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ylkhayat/ECHR-generation-workshop'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_document_into_passages(document_id: str, text: str, max_len: int, overlap: float):\n",
    "    if max_len <= 0:\n",
    "        raise ValueError(\"max_len must be a positive integer\")\n",
    "    if not (0 <= overlap < 1):\n",
    "        raise ValueError(\"overlap must be between 0 (inclusive) and 1 (exclusive)\")\n",
    "\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    step = int(max_len * (1 - overlap))\n",
    "    for i in range(0, len(words), step):\n",
    "        chunk_words = words[i:i+max_len]\n",
    "        if chunk_words:\n",
    "            chunk_text = ' '.join(chunk_words)\n",
    "            chunks.append([document_id, chunk_text])\n",
    "    return chunks\n",
    "\n",
    "def chunk_citations(record):\n",
    "    chunks = []\n",
    "    for citation_id, citation in record['citations']:\n",
    "        chunks.extend(chunk_document_into_passages(citation_id, citation, chunk_size, chunk_overlap))\n",
    "    return chunks\n",
    "\n",
    "if 'oracle_documents_passages' not in current_chosen_dataset.column_names['train'] or 'oracle_documents_passages' not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding oracle_documents_passages to {workshop_hf_name}\")\n",
    "    current_chosen_dataset = current_chosen_dataset.map(lambda record: {'oracle_documents_passages': chunk_citations(record)}, num_proc=num_proc)\n",
    "else:\n",
    "    print(f\"[!] oracle_documents_passages already exists in {workshop_hf_name}\")\n",
    "current_chosen_dataset = current_chosen_dataset.select_columns([key_field, 'previous_text', 'gold_text', 'citations' , 'oracle_documents_passages'])\n",
    "current_chosen_dataset.push_to_hub(workshop_hf_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the BM25 Oracle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "import Stemmer \n",
    "stemmer = Stemmer.Stemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ede8a8f24e418fa36f38468e21d03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/595 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since ylkhayat/ECHR-generation-workshop couldn't be found on the Hugging Face Hub\n",
      "Parameter 'function'=<function retrieve_top_passages at 0x7f13ec151ab0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] ECHR-generation-workshop not found in bm25_oracle_passages_oracle_documents\n",
      "[*] adding top_10_passages to ECHR-generation-workshop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f5a34d6e1544a2b142bce3b7fdec57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6e805c1fdf455090e6db378d66ddb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e3ea0405c7433d8f92f0156c6d3cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f47bed43c9a447db9b272ff8aef4bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a094998d83542f6b3d4f1eb307f5ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26399ba262fa46e3b1b32d850910d1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436b791e785445afab3c46a328d8f86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dd5cb2e37f484d901bd5042dfaa3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef57ab8ca584cb1be9790a80ef649b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730f6274ad2440b7b4c0c7ff68cc0848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46903a18aa5641bd8539cdd909a17ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd86c744f7fb48f6ae5f0d4d4c4012be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2879fbcef9d9402bbecda9b48b515139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/595 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"74969/01\\ncontact with all the persons concerned. It follows from these considerations that the Court's task is not to substitute itself for the domestic authorities in the exercise of their responsibilities regarding custody and access issues, but rather to review, in the light of the Convention, the decisions taken by those authorities in the exercise of their power of appreciation (see Sahin and Sommerfeld v. Germany [GC], nos. 30943/96 and 31871/96, \\u00a7 64 and \\u00a7 62 respectively, ECHR 2003- VIII, and T.P. and K.M. v. the United Kingdom [GC], no. 28945/95, \\u00a7 71, ECHR 2001-V ). 42. The margin of appreciation to be accorded to the competent national authorities will vary in accordance with the nature of the issues and the importance of the interests at stake. In particular when deciding on custody, the Court has recognised that the authorities enjoy a wide margin of appreciation. However, a stricter scrutiny is called for as regards any further limitations, such as restrictions placed by those authorities on parental rights of access, and as regards any legal safeguards designed to secure an effective protection of the right of parents and children to respect for their family life. Such further limitations entail the danger that the family relations between a young child and one or both parents would be effectively curtailed (see Elsholz v. Germany [GC], no. 25735/94, \\u00a7 49, ECHR 2000-VIII, and Kutzner v. Germany, no. 46544/99, \\u00a7 67, ECHR 2002-I ). 43. Article 8 requires that the domestic authorities should strike a fair balance between the interests of the child and those of the parents and that, in the balancing process, particular importance should be attached to the best interests of the child which, depending on their nature and seriousness, may override those of the parents. In particular, a parent cannot be\"\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "data_dir = \"bm25_oracle_passages_oracle_documents\"\n",
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "create = False\n",
    "def retrieve_top_passages(entry):\n",
    "    query = entry['gold_text']\n",
    "    all_passages = entry['oracle_documents_passages']\n",
    "    all_passages_text = [f\"{passage_arr[0]}\\n{passage_arr[1]}\" for passage_arr in all_passages]\n",
    "    corpus_tokens = bm25s.tokenize(all_passages_text, stopwords=\"en\", stemmer=stemmer)\n",
    "    retriever = bm25s.BM25()\n",
    "    retriever.index(corpus_tokens)\n",
    "    query_tokens = bm25s.tokenize(query, stemmer=stemmer)\n",
    "    results, _ = retriever.retrieve(query_tokens, corpus=all_passages_text, k=top_k)\n",
    "    results = results.squeeze(0)\n",
    "    return {f\"top_{top_k}_passages\": results}  \n",
    "try:\n",
    "    new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "except:\n",
    "    print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "    create = True\n",
    "\n",
    "if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "    new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "    new_dataset = new_dataset.map(retrieve_top_passages, num_proc=num_proc)\n",
    "    new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "else:\n",
    "    print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "print(json.dumps(new_dataset['train'][0][f\"top_{top_k}_passages\"][0], indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the BM25 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6318a6eb33544db9b3b427e82b0d57b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/708 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since ylkhayat/ECHR-generation-workshop couldn't be found on the Hugging Face Hub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] ECHR-generation-workshop not found in bm25_relevant_passages_oracle_documents\n",
      "[*] adding top_10_passages to ECHR-generation-workshop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f313d4ff88c49979bf160164912c0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a998628606d4b518ae2f36ddc0ced03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c860e309c06d4dd8b07b309cd0bc5a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cc932d565b4afbb39a48aeee3d9dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbc39e95c6f4f0d9bab843ff90b6a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900d494c433a4d199af317c91377c5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc439406b8b74f2ab2d6d25a0a9569f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb40fd55bac64035bf9740aba0c2e407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c4999aaf92403d871c30cb2c4ca214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d797fa93d9544ede8054aafa3777a34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248bd854c2504acfa4eac27cdc828eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c891b8b5884b7c99dec7147ea44c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b44b5e7499143d3b7dd3ed3a369ee10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/708 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"74969/01\\nTHIRD SECTION CASE OF G\\u00d6RG\\u00dcL\\u00dc v. GERMANY (Application no. 74969/01) FINAL 26/05/2004 JUDGMENT This version was rectified in accordance with Rule 81 of the Rules of Court on 24 May 2005 STRASBOURG 26 February 2004 This judgment will become final in the circumstances set out in Article 44 \\u00a7 2 of the Convention. It may be subject to editorial revision. In the case of G\\u00f6rg\\u00fcl\\u00fc v. Germany, The European Court of Human Rights (Third Section), sitting as a Chamber composed of: Mr L. Caflisch, President, Mr G. Ress, Mr P. K\\u016bris, Mr B. Zupan\\u010di\\u010d, Mr J. Hedigan, Mrs M. Tsatsa-Nikolovska, Mr K. Traja, judges, and Mr V. Berger, Section Registrar, Having deliberated in private on 20 March 2003 and 5 February 2004, Delivers the following judgment, which was adopted on the last \\u2011 mentioned date: PROCEDURE 1. The case originated in an application (no. 74969/01) against the Federal Republic of Germany lodged with the Court under Article 34 of the Convention for the Protection of Human Rights and Fundamental Freedoms (\\u201cthe Convention\\u201d) by a Turkish national of Zaza origin, Kazim G\\u00f6rg\\u00fcl\\u00fc (\\u201cthe applicant\\u201d), on 18 September 2001. 2. The applicant, who had been granted legal aid, was represented by Ms A. Zeycan, a lawyer practising in Bochum. After admissibility he was also represented by Mr P. Koeppel, a lawyer practising in Munich. The German Government (\\u201cthe Government\\u201d) were represented by their Agent, Mr K. Stoltenberg, Ministerialdirigent. 3. The applicant alleged in particular that a court decision refusing him access to and custody of his son violated his right to respect for his family life under Article 8 of the Convention. He also complained about the unfairness of the court proceedings under Article 6 \\u00a7 1 of the Convention. 4. The application was allocated to the Third Section of the Court\"\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "data_dir = \"bm25_relevant_passages_oracle_documents\"\n",
    "\n",
    "create = False\n",
    "def retrieve_top_passages(entry):\n",
    "    query = entry['previous_text']\n",
    "    all_passages = entry['oracle_documents_passages']\n",
    "    all_passages_text = [f\"{passage_arr[0]}\\n{passage_arr[1]}\" for passage_arr in all_passages]\n",
    "    corpus_tokens = bm25s.tokenize(all_passages_text, stopwords=\"en\", stemmer=stemmer)\n",
    "    retriever = bm25s.BM25()\n",
    "    retriever.index(corpus_tokens)\n",
    "    query_tokens = bm25s.tokenize(query, stemmer=stemmer)\n",
    "    results, _ = retriever.retrieve(query_tokens, corpus=all_passages_text, k=top_k)\n",
    "    results = results.squeeze(0)\n",
    "    return {f\"top_{top_k}_passages\": results}  \n",
    "\n",
    "try:\n",
    "    new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "except:\n",
    "    print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "    create = True\n",
    "    \n",
    "if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "    new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "    new_dataset = new_dataset.map(retrieve_top_passages, num_proc=num_proc)\n",
    "    new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "else:\n",
    "    print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "print(json.dumps(new_dataset['train'][0][f\"top_{top_k}_passages\"][0], indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dense Oracle Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dense Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "encoder_name = \"jhu-clsp/LegalBERT-DPR-CLERC-ft\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n",
    "model = AutoModel.from_pretrained(encoder_name)\n",
    "# model = torch.compile(model)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "clean_encoder_name = encoder_name.replace(\"/\", \"_\")\n",
    "data_dir = \"dense_oracle_passages_oracle_documents\"\n",
    "data_dir = f\"{data_dir}/{clean_encoder_name}\"\n",
    "\n",
    "def normalize_embeddings(embeddings):\n",
    "    return embeddings / torch.norm(embeddings, p=2, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "def embed_texts(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=model.config.max_position_embeddings).to(device)\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        token_embeddings = model(**inputs).last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        embeddings = sum_embeddings / sum_mask\n",
    "    embeddings = normalize_embeddings(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def retrieve_top_passages_batch(entries):\n",
    "    all_queries = entries['gold_text']\n",
    "    all_passages_batch = entries['oracle_documents_passages']\n",
    "    all_passages_text = [passage[1] for passages in all_passages_batch for passage in passages]\n",
    "    query_embeddings = embed_texts(all_queries, tokenizer, model)  # Shape: (batch_size, dim)\n",
    "    passage_embeddings = embed_texts(all_passages_text, tokenizer, model)  # Shape: (total_passages, dim)\n",
    "    dim = passage_embeddings.size(1)\n",
    "    # index = faiss.IndexFlatL2(dim)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(passage_embeddings.cpu().numpy())  # FAISS requires NumPy arrays here\n",
    "    distances, indices = index.search(query_embeddings.cpu().numpy(), top_k)\n",
    "    top_passages_batch = [[all_passages_text[index] for index in index_arr] for index_arr in indices]\n",
    "    return {f\"top_{top_k}_passages\": top_passages_batch}\n",
    "\n",
    "try:\n",
    "    new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "except:\n",
    "    print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "    create = True\n",
    "    \n",
    "\n",
    "if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "    new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "    new_dataset = new_dataset.map(\n",
    "        lambda batch: retrieve_top_passages_batch(batch),\n",
    "        batched=True,\n",
    "        batch_size=4\n",
    "    )\n",
    "    new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "else:\n",
    "    print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "print(json.dumps(new_dataset['train'][0][f\"top_{top_k}_passages\"][0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_bm25_oracle_passages_oracle_documents = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"bm25_oracle_passages_oracle_documents\")\n",
    "print(\"===================================\")\n",
    "print(dataset_bm25_oracle_passages_oracle_documents['train'][0]['gold_text'])\n",
    "print(dataset_bm25_oracle_passages_oracle_documents['train'][0]['top_10_passages'][0])\n",
    "\n",
    "dataset_bm25_relevant_passages_oracle_documents = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"bm25_relevant_passages_oracle_documents\")\n",
    "print(\"===================================\")\n",
    "print(dataset_bm25_relevant_passages_oracle_documents['train'][0]['previous_text'])\n",
    "print(dataset_bm25_relevant_passages_oracle_documents['train'][0]['top_10_passages'][0])\n",
    "\n",
    "dataset_dense_oracle_passages_oracle_documents = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"dense_oracle_passages_oracle_documents\")\n",
    "print(\"===================================\")\n",
    "print(dataset_dense_oracle_passages_oracle_documents['train'][0]['gold_text'])\n",
    "print(dataset_dense_oracle_passages_oracle_documents['train'][0]['top_10_passages'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# import faiss\n",
    "# import numpy as np\n",
    "\n",
    "# encoder_name = \"jhu-clsp/LegalBERT-DPR-CLERC-ft\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n",
    "# model = AutoModel.from_pretrained(encoder_name)\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# top_k = 10\n",
    "# data_dir = \"dense_oracle_passages_oracle_documents\"\n",
    "\n",
    "# def embed_passages(passages, tokenizer, model):\n",
    "#     inputs = tokenizer(passages, return_tensors=\"pt\", padding=True, truncation=True, max_length=model.config.max_position_embeddings).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         embeddings = model(**inputs).pooler_output\n",
    "#     return embeddings.cpu().numpy()\n",
    "\n",
    "# def retrieve_top_passages(entry):\n",
    "#     query = entry['gold_text']\n",
    "#     all_passages = entry['oracle_documents_passages']\n",
    "#     all_passages_text = [f\"{passage_arr[0]}\\n{passage_arr[1]}\" for passage_arr in all_passages]\n",
    "    \n",
    "#     passage_embeddings = embed_passages(all_passages_text, tokenizer, model)\n",
    "#     query_embedding = embed_passages([query], tokenizer, model).squeeze(0)\n",
    "\n",
    "#     dim = passage_embeddings.shape[1]\n",
    "#     index = faiss.IndexFlatL2(dim)\n",
    "#     index.add(passage_embeddings)\n",
    "    \n",
    "#     # Retrieve top-k similar passages\n",
    "#     distances, indices = index.search(np.expand_dims(query_embedding, axis=0), top_k)\n",
    "#     top_passages = [all_passages_text[idx] for idx in indices[0]]\n",
    "    \n",
    "#     return {f\"top_{top_k}_passages\": top_passages}\n",
    "\n",
    "# try:\n",
    "#     workshop_hf_name = f\"CLERC-generation-workshop\"\n",
    "#     new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "# except:\n",
    "#     print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "#     create = True\n",
    "    \n",
    "# if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "#     print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "#     new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "#     new_dataset = new_dataset.map(retrieve_top_passages, num_proc=num_proc)\n",
    "#     new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "# else:\n",
    "#     print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "# print(json.dumps(new_dataset['train'][0][f\"top_{top_k}_passages\"][0], indent=4)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
