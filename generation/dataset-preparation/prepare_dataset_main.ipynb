{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import argparse\n",
    "import bm25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 300\n",
    "chunk_overlap = 0.5\n",
    "\n",
    "num_proc = os.cpu_count() - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd909dcb14045b8b4d056adb4146ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b564fb6bb108405aa41739b256099f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/15.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedacbf94d66427f839945baf2a99cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00002.parquet:   0%|          | 0.00/46.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba5f870507c41f884f0250d1bfe69e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00001-of-00002.parquet:   0%|          | 0.00/49.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a586dae77c440f90457a25a09500f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab20bfee810c4680af923eab248ce5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4182 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = \"cuad\"\n",
    "\n",
    "key_field = \"docid\"\n",
    "if dataset == \"clerc\":\n",
    "    original_dataset = load_dataset(\"jhu-clsp/CLERC\", data_files={\"train\": f\"generation/train.jsonl\",  \"test\": f\"generation/test.jsonl\"})\n",
    "    workshop_hf_name = f\"CLERC-generation-workshop\"\n",
    "elif dataset == \"echr\":\n",
    "    workshop_hf_name = f\"ECHR-generation-workshop\"\n",
    "elif dataset == \"echr_qa\":\n",
    "    workshop_hf_name = f\"ECHR_QA-generation-workshop\"\n",
    "elif dataset == \"obli_qa\":\n",
    "    workshop_hf_name = f\"OBLI_QA-generation-workshop\"\n",
    "elif dataset == \"cuad\":\n",
    "    workshop_hf_name = f\"CUAD-generation-workshop\"\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset\")\n",
    "current_chosen_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['docid', 'previous_text', 'gold_text', 'citations', 'oracle_documents_passages'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['docid', 'previous_text', 'gold_text', 'citations', 'oracle_documents_passages'],\n",
       "        num_rows: 4182\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_chosen_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chunk_document_into_passages_overlap_split(document_id: str, text: str, max_len: int, overlap: float):\n",
    "#     if max_len <= 0:\n",
    "#         raise ValueError(\"max_len must be a positive integer\")\n",
    "#     if not (0 <= overlap < 1):\n",
    "#         raise ValueError(\"overlap must be between 0 (inclusive) and 1 (exclusive)\")\n",
    "\n",
    "#     words = text.split()\n",
    "#     chunks = []\n",
    "#     step = int(max_len * (1 - overlap))\n",
    "#     last_index = 0\n",
    "#     for index, i in enumerate(range(0, len(words), step)):\n",
    "#         current_splits = [document_id]\n",
    "#         if i == 0:\n",
    "#             second_split = words[i:i+max_len]\n",
    "#             last_index = i+max_len\n",
    "#             splits = [\"\", \" \".join(second_split)]\n",
    "#         else:\n",
    "#             first_split = words[last_index-step:last_index]\n",
    "#             second_split = words[last_index:last_index+step]\n",
    "#             last_index = last_index+step\n",
    "#             splits = [\" \".join(first_split), \" \".join(second_split)]\n",
    "#         if splits:\n",
    "#             current_splits.extend(splits)\n",
    "#             chunks.append(current_splits)\n",
    "#     return chunks\n",
    "\n",
    "# def chunk_citations_overlap_split(record):\n",
    "#     chunks = []\n",
    "#     for citation_id, citation in record['citations']:\n",
    "#         chunks.extend(chunk_document_into_passages_overlap_split(citation_id, citation, chunk_size, chunk_overlap))\n",
    "#     return chunks\n",
    "\n",
    "# if 'oracle_documents_passages_overlap_split' not in current_chosen_dataset.column_names['train'] or 'oracle_documents_passages_overlap_split' not in current_chosen_dataset.column_names['test']:\n",
    "#     print(f\"[*] adding oracle_documents_passages_overlap_split to {workshop_hf_name}\")\n",
    "#     if \"citations\" not in current_chosen_dataset.column_names['train'] or \"citations\" not in current_chosen_dataset.column_names['test']:\n",
    "#         current_chosen_dataset = DatasetDict({\n",
    "#             'train': current_chosen_dataset['train'].add_column(name=\"citations\", column=original_dataset['train']['citations']),\n",
    "#             'test': current_chosen_dataset['test'].add_column(name=\"citations\", column=original_dataset['test']['citations'])\n",
    "#         })\n",
    "#     current_chosen_dataset = current_chosen_dataset.map(lambda record: {'oracle_documents_passages_overlap_split': chunk_citations_overlap_split(record)})\n",
    "#     current_chosen_dataset = current_chosen_dataset.select_columns(['docid', 'previous_text', 'gold_text', 'citations' ,'oracle_documents_passages', 'oracle_documents_passages_overlap_split'])\n",
    "#     current_chosen_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\")\n",
    "# else:\n",
    "#     print(f\"[!] oracle_documents_passages_overlap_split already exists in {workshop_hf_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] adding oracle_documents_passages to ECHR_QA-generation-workshop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ad346b805e43f5849fe462452a648c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943b83845366424a907bc8f10ac190f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5148ea938eee418c9c9165a77a276034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c74c296a24542afae587a88026f3226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b47032cf1734ca9b6d789a4d580bf75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47017ffcbf3545b8b9b8f10cfda04f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f985837fa8b4520a110484b5a0cee81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f708afdf1b67477e8f11503f79d91e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a247a57e1e3145669249f6e78b43363f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ylkhayat/ECHR_QA-generation-workshop/commit/b7be662d71666090f46b06c362be4597fbb7306f', commit_message='Upload dataset', commit_description='', oid='b7be662d71666090f46b06c362be4597fbb7306f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ylkhayat/ECHR_QA-generation-workshop', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ylkhayat/ECHR_QA-generation-workshop'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_document_into_passages(document_id: str, text: str, max_len: int, overlap: float):\n",
    "    if max_len <= 0:\n",
    "        raise ValueError(\"max_len must be a positive integer\")\n",
    "    if not (0 <= overlap < 1):\n",
    "        raise ValueError(\"overlap must be between 0 (inclusive) and 1 (exclusive)\")\n",
    "\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    step = int(max_len * (1 - overlap))\n",
    "    for i in range(0, len(words), step):\n",
    "        chunk_words = words[i:i+max_len]\n",
    "        if chunk_words:\n",
    "            chunk_text = ' '.join(chunk_words)\n",
    "            chunks.append([document_id, chunk_text])\n",
    "    return chunks\n",
    "\n",
    "def chunk_citations(record):\n",
    "    chunks = []\n",
    "    for citation_id, citation in record['citations']:\n",
    "        chunks.extend(chunk_document_into_passages(citation_id, citation, chunk_size, chunk_overlap))\n",
    "    return chunks\n",
    "\n",
    "if 'oracle_documents_passages' not in current_chosen_dataset.column_names['train'] or 'oracle_documents_passages' not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding oracle_documents_passages to {workshop_hf_name}\")\n",
    "    current_chosen_dataset = current_chosen_dataset.map(lambda record: {'oracle_documents_passages': chunk_citations(record)}, num_proc=num_proc)\n",
    "else:\n",
    "    print(f\"[!] oracle_documents_passages already exists in {workshop_hf_name}\")\n",
    "# current_chosen_dataset = current_chosen_dataset.select_columns([key_field, 'previous_text', 'gold_text', 'citations' , 'oracle_documents_passages'])\n",
    "current_chosen_dataset = current_chosen_dataset.select_columns([key_field, 'previous_text', 'gold_text', 'citations' , 'oracle_documents_passages', 'oracle_documents_oracle_passages'])\n",
    "current_chosen_dataset.push_to_hub(workshop_hf_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the BM25 Oracle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "import Stemmer \n",
    "stemmer = Stemmer.Stemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84d8c3e724a401bb1a49debc38cdc77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/15.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a239c6f953124599bda80da3075f5faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00002.parquet:   0%|          | 0.00/46.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3ee762791b404aa549ca2be2935b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00001-of-00002.parquet:   0%|          | 0.00/48.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45affc3752ad4530a82fc4b1914cb5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read file '/srv/elkhyo/.cache/huggingface/hub/datasets--ylkhayat--CUAD-generation-workshop/snapshots/f4e6f4a4d980ff57bd1a73ae039b502f31636985/bm25_oracle_passages_oracle_documents/train-00000-of-00001.parquet' with error <class 'datasets.table.CastError'>: Couldn't cast\n",
      "id: string\n",
      "title: string\n",
      "context: string\n",
      "question: string\n",
      "answers: struct<text: list<element: string>, answer_start: list<element: int32>>\n",
      "  child 0, text: list<element: string>\n",
      "      child 0, element: string\n",
      "  child 1, answer_start: list<element: int32>\n",
      "      child 0, element: int32\n",
      "docid: string\n",
      "previous_text: string\n",
      "gold_text: string\n",
      "citations: list<element: list<element: string>>\n",
      "  child 0, element: list<element: string>\n",
      "      child 0, element: string\n",
      "oracle_documents_passages: list<element: list<element: string>>\n",
      "  child 0, element: list<element: string>\n",
      "      child 0, element: string\n",
      "top_10_passages: list<element: string>\n",
      "  child 0, element: string\n",
      "-- schema metadata --\n",
      "huggingface: '{\"info\": {\"features\": {\"id\": {\"dtype\": \"string\", \"_type\": \"' + 811\n",
      "to\n",
      "{'docid': Value(dtype='string', id=None), 'previous_text': Value(dtype='string', id=None), 'gold_text': Value(dtype='string', id=None), 'citations': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'oracle_documents_passages': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None)}\n",
      "because column names don't match\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] CUAD-generation-workshop not found in bm25_oracle_passages_oracle_documents\n",
      "[*] adding top_10_passages to CUAD-generation-workshop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f139a9b9fb8464c9ae2a19c0d30d342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09f8b3e0e2d4334884f1b8057ce6ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/4182 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b592d65cde14a5c8c5e93ecbd6722cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8e315508984adb81fe4eae32ebf75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3520792a7054741bdf449051a6edd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2122981235904be6aecddb2d3112c089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248d5ef8754b4a58b7a0b8f31a5e19e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"REFERENCE\\nConflicts between these two languages arising there from, if any, shall be subject to Chinese version. One copy for the Sellers, two copies for the Buyers. The Contract becomes effective after signed by both parties. THE BUYER: THE SELLER: SIGNATURE: SIGNATURE: 6\\n\\nSource: LOHA CO. LTD., F-1, 12/9/2019\"\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "# data_dir = \"bm25_noisy_oracle_passages_oracle_documents\" only for echr_qa\n",
    "data_dir = \"bm25_oracle_passages_oracle_documents\"\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "create = False\n",
    "def retrieve_top_passages(entry):\n",
    "    query = entry['gold_text']\n",
    "    all_passages = entry['oracle_documents_passages']\n",
    "    all_passages_text = [f\"{passage_arr[0]}\\n{passage_arr[1]}\" for passage_arr in all_passages]\n",
    "    corpus_tokens = bm25s.tokenize(all_passages_text, stopwords=\"en\", stemmer=stemmer)\n",
    "    retriever = bm25s.BM25()\n",
    "    retriever.index(corpus_tokens)\n",
    "    query_tokens = bm25s.tokenize(query, stemmer=stemmer)\n",
    "    local_top_k = top_k if top_k <= len(all_passages_text) else len(all_passages_text)\n",
    "    results, _ = retriever.retrieve(query_tokens, corpus=all_passages_text, k=local_top_k)\n",
    "    results = results.squeeze(0)\n",
    "    return {f\"top_{top_k}_passages\": results}  \n",
    "try:\n",
    "    new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "except:\n",
    "    print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "    create = True\n",
    "\n",
    "if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "    new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "    new_dataset = new_dataset.map(retrieve_top_passages, num_proc=num_proc)\n",
    "    new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "else:\n",
    "    print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "print(json.dumps(new_dataset['test'][0][f\"top_{top_k}_passages\"][0], indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the BM25 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf91483d6afe49d7a1a4d7a662f4a4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/16.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f899d2ac8fc148e293b95f5c006ec02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00002.parquet:   0%|          | 0.00/46.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3658d4376e4ae7a210ec649b1a2334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00001-of-00002.parquet:   0%|          | 0.00/48.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ad03c08c314c0e89ef64bd19893e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read file '/srv/elkhyo/.cache/huggingface/hub/datasets--ylkhayat--CUAD-generation-workshop/snapshots/39c4486a7fb5e318f3d9097dcbf18fdd385794ff/bm25_relevant_passages_oracle_documents/train-00000-of-00001.parquet' with error <class 'datasets.table.CastError'>: Couldn't cast\n",
      "docid: string\n",
      "previous_text: string\n",
      "gold_text: string\n",
      "citations: list<element: list<element: string>>\n",
      "  child 0, element: list<element: string>\n",
      "      child 0, element: string\n",
      "oracle_documents_passages: list<element: list<element: string>>\n",
      "  child 0, element: list<element: string>\n",
      "      child 0, element: string\n",
      "top_10_passages: list<element: string>\n",
      "  child 0, element: string\n",
      "-- schema metadata --\n",
      "huggingface: '{\"info\": {\"features\": {\"docid\": {\"dtype\": \"string\", \"_type\"' + 469\n",
      "to\n",
      "{'docid': Value(dtype='string', id=None), 'previous_text': Value(dtype='string', id=None), 'gold_text': Value(dtype='string', id=None), 'citations': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'oracle_documents_passages': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None)}\n",
      "because column names don't match\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] CUAD-generation-workshop not found in bm25_relevant_passages_oracle_documents\n",
      "[*] adding top_10_passages to CUAD-generation-workshop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15944f881ec1438ba116d9d79dbef193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b621cbe01f92428b8de235d651625144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/4182 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_k = 10\n",
    "data_dir = \"bm25_relevant_passages_oracle_documents\"\n",
    "\n",
    "create = False\n",
    "def retrieve_top_passages(entry):\n",
    "    query = entry['previous_text']\n",
    "    all_passages = entry['oracle_documents_passages']\n",
    "    all_passages_text = [f\"{passage_arr[0]}\\n{passage_arr[1]}\" for passage_arr in all_passages]\n",
    "    corpus_tokens = bm25s.tokenize(all_passages_text, stopwords=\"en\", stemmer=stemmer)\n",
    "    retriever = bm25s.BM25()\n",
    "    retriever.index(corpus_tokens)\n",
    "    query_tokens = bm25s.tokenize(query, stemmer=stemmer)\n",
    "    local_top_k = top_k if top_k <= len(all_passages_text) else len(all_passages_text)\n",
    "\n",
    "    results, _ = retriever.retrieve(query_tokens, corpus=all_passages_text, k=local_top_k)\n",
    "    results = results.squeeze(0)\n",
    "    return {f\"top_{top_k}_passages\": results}  \n",
    "\n",
    "try:\n",
    "    new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "except:\n",
    "    print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "    create = True\n",
    "    \n",
    "if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "    new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "    new_dataset = new_dataset.map(retrieve_top_passages, num_proc=num_proc)\n",
    "    new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "else:\n",
    "    print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "print(json.dumps(new_dataset['test'][0][f\"top_{top_k}_passages\"][0], indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dense Oracle Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dense Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "encoder_name = \"jhu-clsp/LegalBERT-DPR-CLERC-ft\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n",
    "model = AutoModel.from_pretrained(encoder_name)\n",
    "# model = torch.compile(model)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "clean_encoder_name = encoder_name.replace(\"/\", \"_\")\n",
    "data_dir = \"dense_oracle_passages_oracle_documents\"\n",
    "data_dir = f\"{data_dir}/{clean_encoder_name}\"\n",
    "\n",
    "def normalize_embeddings(embeddings):\n",
    "    return embeddings / torch.norm(embeddings, p=2, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "def embed_texts(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=model.config.max_position_embeddings).to(device)\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        token_embeddings = model(**inputs).last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        embeddings = sum_embeddings / sum_mask\n",
    "    embeddings = normalize_embeddings(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def retrieve_top_passages_batch(entries):\n",
    "    all_queries = entries['gold_text']\n",
    "    all_passages_batch = entries['oracle_documents_passages']\n",
    "    all_passages_text = [passage[1] for passages in all_passages_batch for passage in passages]\n",
    "    query_embeddings = embed_texts(all_queries, tokenizer, model)  # Shape: (batch_size, dim)\n",
    "    passage_embeddings = embed_texts(all_passages_text, tokenizer, model)  # Shape: (total_passages, dim)\n",
    "    dim = passage_embeddings.size(1)\n",
    "    # index = faiss.IndexFlatL2(dim)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(passage_embeddings.cpu().numpy())  # FAISS requires NumPy arrays here\n",
    "    distances, indices = index.search(query_embeddings.cpu().numpy(), top_k)\n",
    "    top_passages_batch = [[all_passages_text[index] for index in index_arr] for index_arr in indices]\n",
    "    return {f\"top_{top_k}_passages\": top_passages_batch}\n",
    "\n",
    "try:\n",
    "    new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "except:\n",
    "    print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "    create = True\n",
    "    \n",
    "\n",
    "if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "    new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "    new_dataset = new_dataset.map(\n",
    "        lambda batch: retrieve_top_passages_batch(batch),\n",
    "        batched=True,\n",
    "        batch_size=4\n",
    "    )\n",
    "    new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "else:\n",
    "    print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "print(json.dumps(new_dataset['train'][0][f\"top_{top_k}_passages\"][0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_bm25_oracle_passages_oracle_documents = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"bm25_oracle_passages_oracle_documents\")\n",
    "print(\"===================================\")\n",
    "print(dataset_bm25_oracle_passages_oracle_documents['train'][0]['gold_text'])\n",
    "print(dataset_bm25_oracle_passages_oracle_documents['train'][0]['top_10_passages'][0])\n",
    "\n",
    "dataset_bm25_relevant_passages_oracle_documents = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"bm25_relevant_passages_oracle_documents\")\n",
    "print(\"===================================\")\n",
    "print(dataset_bm25_relevant_passages_oracle_documents['train'][0]['previous_text'])\n",
    "print(dataset_bm25_relevant_passages_oracle_documents['train'][0]['top_10_passages'][0])\n",
    "\n",
    "dataset_dense_oracle_passages_oracle_documents = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"dense_oracle_passages_oracle_documents\")\n",
    "print(\"===================================\")\n",
    "print(dataset_dense_oracle_passages_oracle_documents['train'][0]['gold_text'])\n",
    "print(dataset_dense_oracle_passages_oracle_documents['train'][0]['top_10_passages'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# import faiss\n",
    "# import numpy as np\n",
    "\n",
    "# encoder_name = \"jhu-clsp/LegalBERT-DPR-CLERC-ft\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n",
    "# model = AutoModel.from_pretrained(encoder_name)\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# top_k = 10\n",
    "# data_dir = \"dense_oracle_passages_oracle_documents\"\n",
    "\n",
    "# def embed_passages(passages, tokenizer, model):\n",
    "#     inputs = tokenizer(passages, return_tensors=\"pt\", padding=True, truncation=True, max_length=model.config.max_position_embeddings).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         embeddings = model(**inputs).pooler_output\n",
    "#     return embeddings.cpu().numpy()\n",
    "\n",
    "# def retrieve_top_passages(entry):\n",
    "#     query = entry['gold_text']\n",
    "#     all_passages = entry['oracle_documents_passages']\n",
    "#     all_passages_text = [f\"{passage_arr[0]}\\n{passage_arr[1]}\" for passage_arr in all_passages]\n",
    "    \n",
    "#     passage_embeddings = embed_passages(all_passages_text, tokenizer, model)\n",
    "#     query_embedding = embed_passages([query], tokenizer, model).squeeze(0)\n",
    "\n",
    "#     dim = passage_embeddings.shape[1]\n",
    "#     index = faiss.IndexFlatL2(dim)\n",
    "#     index.add(passage_embeddings)\n",
    "    \n",
    "#     # Retrieve top-k similar passages\n",
    "#     distances, indices = index.search(np.expand_dims(query_embedding, axis=0), top_k)\n",
    "#     top_passages = [all_passages_text[idx] for idx in indices[0]]\n",
    "    \n",
    "#     return {f\"top_{top_k}_passages\": top_passages}\n",
    "\n",
    "# try:\n",
    "#     workshop_hf_name = f\"CLERC-generation-workshop\"\n",
    "#     new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "# except:\n",
    "#     print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "#     create = True\n",
    "    \n",
    "# if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "#     print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "#     new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "#     new_dataset = new_dataset.map(retrieve_top_passages, num_proc=num_proc)\n",
    "#     new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "# else:\n",
    "#     print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "# print(json.dumps(new_dataset['train'][0][f\"top_{top_k}_passages\"][0], indent=4)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
