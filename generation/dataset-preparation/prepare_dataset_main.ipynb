{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import argparse\n",
    "import bm25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 300\n",
    "chunk_overlap = 0.0\n",
    "\n",
    "num_proc = os.cpu_count() - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3638e7b57c7a4c5083bf1e690c9c9fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/534 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a919d6afd449a2a795e58499005c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf5648030ea49d7a169f8e61ab5bfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/35.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdc2193855b4b09a7236941f80238c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2124719e5ed047a58d6d05f54248fe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = \"oal_qa\"\n",
    "\n",
    "key_field = \"docid\"\n",
    "if dataset == \"clerc\":\n",
    "    original_dataset = load_dataset(\"jhu-clsp/CLERC\", data_files={\"train\": f\"generation/train.jsonl\",  \"test\": f\"generation/test.jsonl\"})\n",
    "    workshop_hf_name = f\"CLERC-generation-workshop\"\n",
    "elif dataset == \"echr\":\n",
    "    workshop_hf_name = f\"ECHR-generation-workshop\"\n",
    "elif dataset == \"echr_qa\":\n",
    "    workshop_hf_name = f\"ECHR_QA-generation-workshop\"\n",
    "elif dataset == \"obli_qa\":\n",
    "    workshop_hf_name = f\"OBLI_QA-generation-workshop\"\n",
    "elif dataset == \"cuad\":\n",
    "    workshop_hf_name = f\"CUAD-generation-workshop\"\n",
    "elif dataset == \"oal_qa\":\n",
    "    workshop_hf_name = f\"OAL_QA-generation-workshop\"\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset\")\n",
    "current_chosen_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['docid', 'previous_text', 'gold_text', 'citations'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['docid', 'previous_text', 'gold_text', 'citations'],\n",
       "        num_rows: 2024\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_chosen_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chunk_document_into_passages_overlap_split(document_id: str, text: str, max_len: int, overlap: float):\n",
    "#     if max_len <= 0:\n",
    "#         raise ValueError(\"max_len must be a positive integer\")\n",
    "#     if not (0 <= overlap < 1):\n",
    "#         raise ValueError(\"overlap must be between 0 (inclusive) and 1 (exclusive)\")\n",
    "\n",
    "#     words = text.split()\n",
    "#     chunks = []\n",
    "#     step = int(max_len * (1 - overlap))\n",
    "#     last_index = 0\n",
    "#     for index, i in enumerate(range(0, len(words), step)):\n",
    "#         current_splits = [document_id]\n",
    "#         if i == 0:\n",
    "#             second_split = words[i:i+max_len]\n",
    "#             last_index = i+max_len\n",
    "#             splits = [\"\", \" \".join(second_split)]\n",
    "#         else:\n",
    "#             first_split = words[last_index-step:last_index]\n",
    "#             second_split = words[last_index:last_index+step]\n",
    "#             last_index = last_index+step\n",
    "#             splits = [\" \".join(first_split), \" \".join(second_split)]\n",
    "#         if splits:\n",
    "#             current_splits.extend(splits)\n",
    "#             chunks.append(current_splits)\n",
    "#     return chunks\n",
    "\n",
    "# def chunk_citations_overlap_split(record):\n",
    "#     chunks = []\n",
    "#     for citation_id, citation in record['citations']:\n",
    "#         chunks.extend(chunk_document_into_passages_overlap_split(citation_id, citation, chunk_size, chunk_overlap))\n",
    "#     return chunks\n",
    "\n",
    "# if 'oracle_documents_passages_overlap_split' not in current_chosen_dataset.column_names['train'] or 'oracle_documents_passages_overlap_split' not in current_chosen_dataset.column_names['test']:\n",
    "#     print(f\"[*] adding oracle_documents_passages_overlap_split to {workshop_hf_name}\")\n",
    "#     if \"citations\" not in current_chosen_dataset.column_names['train'] or \"citations\" not in current_chosen_dataset.column_names['test']:\n",
    "#         current_chosen_dataset = DatasetDict({\n",
    "#             'train': current_chosen_dataset['train'].add_column(name=\"citations\", column=original_dataset['train']['citations']),\n",
    "#             'test': current_chosen_dataset['test'].add_column(name=\"citations\", column=original_dataset['test']['citations'])\n",
    "#         })\n",
    "#     current_chosen_dataset = current_chosen_dataset.map(lambda record: {'oracle_documents_passages_overlap_split': chunk_citations_overlap_split(record)})\n",
    "#     current_chosen_dataset = current_chosen_dataset.select_columns(['docid', 'previous_text', 'gold_text', 'citations' ,'oracle_documents_passages', 'oracle_documents_passages_overlap_split'])\n",
    "#     current_chosen_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\")\n",
    "# else:\n",
    "#     print(f\"[!] oracle_documents_passages_overlap_split already exists in {workshop_hf_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chunk_document_into_passages(document_id: str, text: str, max_len: int, overlap: float):\n",
    "#     if max_len <= 0:\n",
    "#         raise ValueError(\"max_len must be a positive integer\")\n",
    "#     if not (0 <= overlap < 1):\n",
    "#         raise ValueError(\"overlap must be between 0 (inclusive) and 1 (exclusive)\")\n",
    "\n",
    "#     words = text.split()\n",
    "#     chunks = []\n",
    "#     step = int(max_len * (1 - overlap))\n",
    "#     for i in range(0, len(words), step):\n",
    "#         chunk_words = words[i:i+max_len]\n",
    "#         if chunk_words:\n",
    "#             chunk_text = ' '.join(chunk_words)\n",
    "#             chunks.append([document_id, chunk_text])\n",
    "#     return chunks\n",
    "\n",
    "# def chunk_citations(record):\n",
    "#     chunks = []\n",
    "#     for citation_id, citation in record['citations']:\n",
    "#         chunks.extend(chunk_document_into_passages(citation_id, citation, chunk_size, chunk_overlap))\n",
    "#     return chunks\n",
    "\n",
    "# if 'oracle_documents_passages' not in current_chosen_dataset.column_names['train'] or 'oracle_documents_passages' not in current_chosen_dataset.column_names['test']:\n",
    "#     print(f\"[*] adding oracle_documents_passages to {workshop_hf_name}\")\n",
    "#     current_chosen_dataset = current_chosen_dataset.map(lambda record: {'oracle_documents_passages': chunk_citations(record)}, num_proc=num_proc)\n",
    "# else:\n",
    "#     print(f\"[!] oracle_documents_passages already exists in {workshop_hf_name}\")\n",
    "# # current_chosen_dataset = current_chosen_dataset.select_columns([key_field, 'previous_text', 'gold_text', 'citations' , 'oracle_documents_passages'])\n",
    "# current_chosen_dataset = current_chosen_dataset.select_columns([key_field, 'previous_text', 'gold_text', 'citations' , 'oracle_documents_passages', 'oracle_documents_oracle_passages'])\n",
    "# current_chosen_dataset.push_to_hub(workshop_hf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def chunk_document_into_passages(document_id: str, text: str, max_len: int = 300, overlap: float = 0.0):\n",
    "#     if max_len <= 0:\n",
    "#         raise ValueError(\"max_len must be a positive integer\")\n",
    "#     if not (0 <= overlap < 1):\n",
    "#         raise ValueError(\"overlap must be between 0 (inclusive) and 1 (exclusive)\")\n",
    "\n",
    "#     doc = nlp(text)\n",
    "#     sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "#     chunks = []\n",
    "#     chunk_text = \"\"\n",
    "#     overlap_len = int(overlap * max_len)\n",
    "\n",
    "#     for sentence in sentences:\n",
    "#         to_add = sentence\n",
    "#         while to_add:\n",
    "#             available = max_len - len(chunk_text)\n",
    "#             if len(to_add) <= available:\n",
    "#                 if chunk_text:\n",
    "#                     chunk_text += \" \" + to_add\n",
    "#                 else:\n",
    "#                     chunk_text = to_add\n",
    "#                 to_add = \"\"\n",
    "#             else:\n",
    "#                 part = to_add[:available]\n",
    "#                 remainder = to_add[available:].strip()\n",
    "#                 if chunk_text:\n",
    "#                     chunk_text += \" \" + part\n",
    "#                 else:\n",
    "#                     chunk_text = part\n",
    "#                 chunks.append([document_id, chunk_text])\n",
    "#                 if overlap_len > 0:\n",
    "#                     overlap_str = chunk_text[-overlap_len:].strip()\n",
    "#                     chunk_text = overlap_str\n",
    "#                 else:\n",
    "#                     chunk_text = \"\"\n",
    "#                 to_add = remainder\n",
    "#     if chunk_text:\n",
    "#         chunks.append([document_id, chunk_text])\n",
    "#     return chunks\n",
    "\n",
    "# def chunk_citations(record):\n",
    "#     chunks = []\n",
    "#     for citation_id, citation in record['citations']:\n",
    "#         chunks.extend(chunk_document_into_passages(citation_id, citation, chunk_size, chunk_overlap))\n",
    "#     return chunks\n",
    "\n",
    "# if 'oracle_documents_passages' not in current_chosen_dataset.column_names['train'] or 'oracle_documents_passages' not in current_chosen_dataset.column_names['test']:\n",
    "#     print(f\"[*] adding oracle_documents_passages to {workshop_hf_name}\")\n",
    "#     current_chosen_dataset = current_chosen_dataset.map(lambda record: {'oracle_documents_passages': chunk_citations(record)}, num_proc=num_proc)\n",
    "# else:\n",
    "#     print(f\"[!] oracle_documents_passages already exists in {workshop_hf_name}\")\n",
    "# # current_chosen_dataset = current_chosen_dataset.select_columns([key_field, 'previous_text', 'gold_text', 'citations' , 'oracle_documents_passages'])\n",
    "# current_chosen_dataset = current_chosen_dataset.select_columns([key_field, 'previous_text', 'gold_text', 'citations' , 'oracle_documents_passages'])\n",
    "# current_chosen_dataset.push_to_hub(workshop_hf_name, data_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import windowed\n",
    "import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def chunk_document_into_passages(document_id: str, text: str, max_len: int = 300, overlap: float = 0.0):\n",
    "#     if max_len <= 0:\n",
    "#         raise ValueError(\"max_len must be a positive integer\")\n",
    "#     if not (0 <= overlap < 1):\n",
    "#         raise ValueError(\"overlap must be between 0 (inclusive) and 1 (exclusive)\")\n",
    "        \n",
    "#     doc = nlp(text)\n",
    "#     words = [token.text for token in doc if not token.is_space]\n",
    "\n",
    "#     chunks = []\n",
    "#     chunk_words = []\n",
    "#     current_word_count = 0  # word count of the current chunk\n",
    "#     overlap_count = int(overlap * max_len)\n",
    "\n",
    "#     for word in words:\n",
    "#         if current_word_count + 1 > max_len:\n",
    "#             chunk_text = \" \".join(chunk_words).strip()\n",
    "#             if chunk_text:\n",
    "#                 chunks.append([document_id, chunk_text])\n",
    "#             if overlap_count > 0:\n",
    "#                 overlap_words = chunk_words[-overlap_count:]\n",
    "#                 chunk_words = overlap_words\n",
    "#                 current_word_count = len(chunk_words)\n",
    "#             else:\n",
    "#                 chunk_words = []\n",
    "#                 current_word_count = 0\n",
    "            \n",
    "#             chunk_words.append(word)\n",
    "#             current_word_count += 1\n",
    "#         else:\n",
    "#             chunk_words.append(word)\n",
    "#             current_word_count += 1\n",
    "\n",
    "#     if chunk_words:\n",
    "#         chunk_text = \" \".join(chunk_words).strip()\n",
    "#         if chunk_text:\n",
    "#             chunks.append([document_id, chunk_text])\n",
    "#     return chunks\n",
    "\n",
    "def chunk_document_into_passages(document_id: str, text: str, max_len: int = 300, overlap: float = 0.0):\n",
    "    if max_len <= 0:\n",
    "        raise ValueError(\"max_len must be a positive integer\")\n",
    "    if not (0 <= overlap < 1):\n",
    "        raise ValueError(\"overlap must be between 0 (inclusive) and 1 (exclusive)\")\n",
    "    chunks = []\n",
    "    words = text.split(\" \")\n",
    "    chunked_words = windowed(words, max_len, fillvalue=\"\", step=int(max_len * (1 - overlap)))\n",
    "    for chunk in chunked_words:\n",
    "        chunks.append([document_id, \" \".join(chunk).strip()])\n",
    "    return chunks\n",
    "\n",
    "def chunk_citations(record, chunk_size=300, chunk_overlap=0.0):\n",
    "    chunks = []\n",
    "    for citation_id, citation in record['citations']:\n",
    "        chunks.extend(chunk_document_into_passages(citation_id, citation, max_len=chunk_size, overlap=chunk_overlap))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"nsw_caselaw:549fc6183004262463bb648a\",\n",
      "        \"New South Wales\\nSupreme Court\\n  CITATION :         Nasr v NRMA Insurance [2006] NSWSC 1018\\n  HEARING DATE(S) :  27 September 2006\\n  JUDGMENT DATE :    29 September 2006\\n  JURISDICTION :     Common Law Division\\n  JUDGMENT OF :      Associate Justice Harrison\\n  DECISION :         (1) The appeal is dismissed; (2) The orders of the Magistrate Lulham dated 4 October 2005 are affirmed; (3) The summons filed 8 June 2006 is dismissed; (4) The plaintiff is to pay the defendant's costs as agreed or assessed.\\n  CATCHWORDS :         Appeal decision of Local Court Magistrate - non-appearance - Statement of Claim struck out\\n  LEGISLATION CITED :  Local Courts Act 1982 - s 73\\n                       Allan v Kerr & Anor (1995) Aust Torts Reports 81-354\\n                       Azzopardi v Tasman UEB Industries Ltd (1985) 4 NSWLR 139\\n                       Carr v Neill [1999] NSWSC 1263\\n                       Devries v Australian National Railways Commission (1993) 177 CLR 472\\n  CASES CITED :        Kearns & Anor v Fair Trading Tribunal of NSW & Anor [2001] NSWSC 951\"\n",
      "    ],\n",
      "    [\n",
      "        \"nsw_caselaw:549fc6183004262463bb648a\",\n",
      "        \"Kojima Australia Pty Ltd v Australian Chinese Newspapers Pty Ltd [2002] NSWSC 1153\\n                       R L & D Investments Pty Ltd v Bisby (2002) 37 MVR 479; [2002] NSWSC 1082\\n                       State Rail Authority of New South Wales v Earthline Constructions Pty Ltd (in Liq) (1999) 160 ALR 588\\n                       Wakim v Mathiew Pty Ltd t/as Dove Migration Services [2002] NSWSC 405\\n  PARTIES :            Nabil Nasr - Plaintiff\\n                       NRMA Insurance Limited - Defendant\\n  FILE NUMBER(S) :     SC 12940/2006\\n  COUNSEL :            Ms K E Day - Defendant\\n  SOLICITORS :         Mr N Nasr - Plaintiff in person\\n                       Mason Black Lawyers - Defendant\\n  LOWER COURT JURISDICTION :      Local Court\\n  LOWER COURT FILE NUMBER(S) :    9222/04\\n  LOWER COURT JUDICIAL OFFICER :  Lulham LCM\\n  LOWER COURT\"\n",
      "    ],\n",
      "    [\n",
      "        \"nsw_caselaw:549fc6183004262463bb648a\",\n",
      "        \"DATE OF DECISION :  4 October 2005\\n          IN THE SUPREME COURT\\n          OF NEW SOUTH WALES\\n          COMMON LAW DIVISION\\n          ASSOCIATE JUSTICE HARRISON\\n          FRIDAY, 29 SEPTEMBER 2006\\n          12940/2006 - NABIL NASR v NRMA INSURANCE\\n                          LIMITED\\n          JUDGMENT (Appeal decision of Local Court Magistrate\\n          - non-appearance - Statement of Claim\\n                          struck out)\\n  1 HER HONOUR: By summons filed 8 June 2006, the plaintiff Nabil Nasr seeks to appeal the decision of His Honour Lulham LCM dated 4 October 2005. Mr Nasr claims that he was overseas when the Local Court struck out his case against the defendant, NRMA Insurance Limited (NRMA Insurance). NRMA Insurance rejected payment of Mr Nasr\\u2019s claim for his car after it was burnt on 6 July 2004. Mr Nasr appeared unrepresented and has written a letter dated 23 September 2006. NRMA Insurance relied on the affidavit of M Candan Koyuncu dated 13 July 2006.\\n  2 At the outset, it may be helpful to make some brief comments concerning the remedy pursued by the plaintiff. Section 73 of the Local Courts Act 1982 (formerly Act No\"\n",
      "    ],\n",
      "    [\n",
      "        \"nsw_caselaw:549fc6183004262463bb648a\",\n",
      "        \"11 1970, section 69 (2) and (2A)) permits a party who is dissatisfied with a judgment as being erroneous in point of law to appeal to this Court. The onus lies on the plaintiff to demonstrate that there has been an error of law. What is a question of law (as opposed to a question of fact) was considered, inter alia, in Allen v Kerr & Anor (1995) Aust Torts Reports 81-354; Azzopardi v Tasman UEB Industries Ltd (1985) 4 NSWLR 139 at 155-156; Carr v Neill [1999] NSWSC 1263; and R L & D Investments Pty Ltd v Bisby (2002) 37 MVR 479; [2002] NSWSC 1082. It cannot be said that the judicial officer acted on evidence inconsistent with facts incontrovertibly established by the evidence - see Devries v Australian National Railways Commission (1993) 177 CLR 472 per Brennan, Gaudron and McHugh JJ at 479 and State Rail Authority of New South Wales v Earthline Constructions Pty Ltd (in Liq) (1999) 160 ALR 588.\\n          Grounds of appeal and extension of time to file the appeal\\n  3 The plaintiff claims that he was overseas when the Local Court struck out his case against the NRMA and they (the NRMA) rejected payment of his claim for his car after it was burnt on 6 July 2004. There are no grounds of appeal in his summons but it may be that he could have submitted that he was denied procedural fairness or natural justice.\\n  4 This appeal has been lodged out of time. The decision of the Local Court was made on 4 October 2005. The summons was filed on 8 June 2006, some seven months out of time. No explanation has been provided for this delay. In these circumstances this\"\n",
      "    ],\n",
      "    [\n",
      "        \"nsw_caselaw:549fc6183004262463bb648a\",\n",
      "        \"Court cannot grant an extension of time in which to lodge this appeal.\\n          The Local Court proceedings\\n  5 The Local Court file was not before this Court. There are four letters from the Local Court in evidence. The statement of claim is not before this Court. However, it seems that Mr Nasr sued the NRMA because it denied to pay a claim he made pursuant to his motor vehicle insurance policy and he was seeking damages. Doing the best I can, it appears that Mr Nasr sought and was granted a number of adjournments while he was overseas in China.\\n  6 It appears that the first review date in the Local Court was scheduled for 18 January 2005. As that hearing was to occur while Mr Nasr was overseas, he sent a friend to request an adjournment or represent him. Mr Nasr recounted that the Court advised his friend that he could not represent Mr Nasr and that Mr Nasr or a lawyer should appear. Mr Nasr faxed to the Court a request for an adjournment. The Court acknowledged the fax by email.\\n  7 On 18 January 2005 the Local Court made an order adjourning the matter to 12 April 2005 and ordered that the plaintiff was to pay the defendant\\u2019s costs of $162.00 within 28 days. The Court forwarded a letter to Mr Nasr informing him of this result and advising him that he should be aware that if he did not appear at the Court at the time set down, the Court may make a decision in his absence.\\n  8 On 12 April 2005 the plaintiff did not appear at court and the following orders were made.\"\n",
      "    ],\n",
      "    [\n",
      "        \"nsw_caselaw:549fc6183004262463bb648a\",\n",
      "        \"(1) Statement of claim struck out following the non appearance of the plaintiff.\\n                  (2) Judgement for defendant for costs incurred\\n                  $324.00 for the 26/11/2004\\n                  $162.00 for the 18/01/2005\\n                  $324.00 costs of today\\n                  (3) Total amount owing $810.00 payable within 28 days.\\n  9 This order striking out the statement of claim appears to have been set aside and the statement of claim was restored to the list and relisted on 20 September 2005.\\n  10 On 20 September 2005 the matter was adjourned to 4 October 2005 for further review and the plaintiff was ordered to pay the defendant\\u2019s costs in the sum of $300 within 21 days. At this time the plaintiff was legally represented by his solicitors.\\n  11 On 21 September 2005 the NRMA solicitor Mr Koyuncu wrote to Macquarie Lawyers, who were acting for Mr Nasr and confirmed that he appeared before Mr Lulham LCM for the review of this matter. While he was waiting for the matter to be called he telephoned a representative of Macquarie Lawyers who advised him that a solicitor would appear at the mention. This did not happen. Later, Elias from Macquarie Lawyers contacted Mr Koyuncu and informed that they were not able to obtain instructions from\"\n",
      "    ],\n",
      "    [\n",
      "        \"nsw_caselaw:549fc6183004262463bb648a\",\n",
      "        \"the plaintiff and would be filing a Notice of Discontinuance. Mr Koyuncu advised the Magistrate of this conversation and the following orders were made:\\n                  1. The review of the matter adjourned to 4 October 2005;\\n                  2. The plaintiff to pay the defendant\\u2019s costs of $300.00;\\n                  3. The defendant to advise the plaintiff and the plaintiff\\u2019s solicitor of the above orders.\\n  12 The NRMA solicitor wrote to Mr Nasr informing him of the orders made on 20 September 2005 (Ex 1). Apparently, Mr Nasr was informed by his solicitors that, without payment, they would not appear for him again. Macquarie Lawyers still hold his file and he owes them $1,800.\\n  13 On 4 October 2005 there was no appearance by Mr Nasr. The Court wrote to Mr Nasr informing him that the statement of claim was struck out and that he had been ordered to pay the defendant\\u2019s costs thrown away. This notice further stated \\u201cNotice of Motion to be filed by the plaintiff to reinstate\\u201d.\\n          The Law\\n  14 In Wakim v Mathiew Pty Ltd t/as Dove Migration Services [2002] NSWSC 405, O\\u2019Keefe J made the following observations:\\n                  \\u201c20 The requirements of natural justice (or procedural fairness as it is now commonly referred to) apply to the Small Claims Division of the Local Courts. This is clear from the nature of\"\n",
      "    ],\n",
      "    [\n",
      "        \"nsw_caselaw:549fc6183004262463bb648a\",\n",
      "        \"the function to be performed by that tribunal and the statutory recognition that is afforded to natural justice by s 69(2A) of the Act.\\n                  21 The content of the requirements of natural justice is not fixed. The content fluctuates. The overarching requirement is that of fairness (National Companies and Securities Commission v News Corporation Ltd (1984) 156 CLR at 312 per Gibbs CJ with whom Brennan J agreed). For a court that normally involves a duty to:\\n                  (i) act judicially;\\n                  (ii) deal with the matter for decision without bias;\\n                  (iii) give each party the opportunity of adequately presenting its case;\\n                  (iv) observe the procedural and other rules provided for in the relevant statute;\\n                  (v) come to its decision with that sense of responsibility that is the necessary accompaniment of the duty to do justice.\\u201d\\n  15 O\\u2019Keefe J made the same comments in Kojima Australia Pty Ltd v Australian Chinese Newspapers Pty Ltd [2000] NSWSC 1153 at para 23.\\n  16 In Kearns & Anor v Fair Trading Tribunal of NSW & Anor [2001] NSWSC 951 Grove J stated at para 25:\"\n",
      "    ],\n",
      "    [\n",
      "        \"nsw_caselaw:549fc6183004262463bb648a\",\n",
      "        \"\\u201cThere is ample authority that procedural fairness is denied if a decision maker fails to adjourn proceedings where such a failure has the effect of depriving a person of adequate opportunity to prepare or present a case: Sullivan v Department of Transport (1978) 20 ALR 323; Opitz v Repatriation Commission (1991) 29 FCR 50; Humphrey v Wills (1989) VR 439.\\u201d\\n  17 Mr Nasr had sought and been granted a number of adjournments prior to 4 October 2005. While Mr Nasr did not appear at Court on 4 October 2005, it is not clear whether he contacted the Court requesting an adjournment. In this case, Mr Nasr was the plaintiff and as such was responsible for prosecuting his proceedings in a timely manner. Adjournments are not given as a matter of right. He had been granted a number of adjournments and once, when the statement of claim had been struck out, he had been given leave to have it restored. Given that history, it was open to the Magistrate to dismiss the statement of claim when the plaintiff failed to appear at Court on 4 October 2005. There has been no denial of procedural fairness. The appeal is dismissed.\\n  18 The orders of His Honour Magistrate Lulham dated 4 October 2005 are affirmed. The summons filed 8 June 2006 is dismissed.\\n  19 Costs are discretionary. Costs usually follow the event. The plaintiff is to pay the defendant\\u2019s costs as agreed or assessed.\\n          The Court orders:\\n          (1) The appeal is dismissed.\\n          (2) The orders of His Honour Magistrate Lulham dated 4 October 2005 are affirmed.\"\n",
      "    ],\n",
      "    [\n",
      "        \"nsw_caselaw:549fc6183004262463bb648a\",\n",
      "        \"(3) The summons filed 8 June 2006 is dismissed.\\n          (4) The plaintiff is to pay the defendant\\u2019s costs as agreed or assessed.\\n          **********\\nDISCLAIMER - Every effort has been made to comply with suppression orders or statutory provisions prohibiting publication that may apply to this judgment or decision. The onus remains on any person using material in the judgment or decision to ensure that the intended use of that material does not breach any such order or provision. Further enquiries may be directed to the Registry of the Court or Tribunal in which it was generated.\"\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(chunk_citations(current_chosen_dataset['test'][0]), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] adding oracle_documents_passages to OBLI_QA-generation-workshop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3a7557eebc4bb18366e829a26b791b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "create = False\n",
    "if create or 'oracle_documents_passages' not in current_chosen_dataset.column_names['train'] or 'oracle_documents_passages' not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding oracle_documents_passages to {workshop_hf_name}\")\n",
    "    # Use parallel processing with num_proc if dataset is large. Increase num_proc as needed.\n",
    "    current_chosen_dataset = current_chosen_dataset.map(\n",
    "        lambda record: {'oracle_documents_passages': chunk_citations(record)},\n",
    "        num_proc=num_proc\n",
    "    )\n",
    "else:\n",
    "    print(f\"[!] oracle_documents_passages already exists in {workshop_hf_name}\")\n",
    "\n",
    "current_chosen_dataset = current_chosen_dataset.select_columns([key_field, 'previous_text', 'gold_text', 'citations', 'oracle_documents_passages'])\n",
    "current_chosen_dataset.push_to_hub(workshop_hf_name, data_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "import Stemmer \n",
    "stemmer = Stemmer.Stemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the BM25 Oracle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function retrieve_top_passages at 0x7f7031e76c20> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] adding top_10_passages to OBLI_QA-generation-workshop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617e3a5ceab248cb96886de66811c4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2400a47d8e0b42fd9d3d9602e30aae7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/2786 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf9d6cdfce749989ebe8d23c02412f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5b4666db4043fc94f83e4dbf42c9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996617ecdbac4becb3202f94f4c18423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a2d8df379f44b4a334dbf6b586a624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f8b5de3cfa4732b917e0419ef72440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10480bc32de74d6ca0e96b672c65d3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9118ac60417e4f26b901d5de0a8a7bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39363e27a4864d368e4885eac326f834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a3ae15d1b049588d4f03b2ff797acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910058d2c4a442a1ae3bfa7558b7898a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1e734371a74345bdaf4d54d66f6624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1\\n14. SUSPICIOUS ACTIVITY/TRANSACTION REPORTS\\n14.1 Application and definitions\\nIn this Chapter \\\"money laundering\\\" and \\u201cterrorist financing\\\" means the criminal offences defined in Federal AML Legislation.\\n14.2 Internal reporting requirements\\n14.2.1 A Relevant Person must establish and maintain policies, procedures, systems and controls in order to monitor and detect suspicious activity or Transactions in relation to potential money laundering or terrorist financing.\\n14.2.2 A Relevant Person must have policies, procedures, systems and controls to ensure that whenever any Employee, acting in the ordinary course of his employment, either:\\n(a)\\tknows;\\n(b)\\tsuspects; or\\n(c)\\thas reasonable grounds for knowing or suspecting,\\nthat a Person is engaged in or attempting money laundering or terrorist financing, that Employee promptly notifies the Relevant Person's MLRO and provides the MLRO with all relevant details.\\n14.2.3 A Relevant Person must have policies and procedures to ensure that disciplinary action can be taken against any Employee who fails to make such a report.\\n14.2.3.Guidance\\n14.2.3.Guidance.1. Circumstances that might give rise to suspicion or reasonable grounds for suspicion of money laundering or terrorist financing include:\\n(a)\\tTransactions which have no apparent purpose, which make no obvious economic sense, or which are designed or structured to avoid detection;\\n(b)\\tTransactions requested by a Person without reasonable explanation, which are out of the ordinary range of services normally requested or are outside the experience of a Relevant Person in relation to a particular customer;\\n(c)\\twhere the size or pattern of Transactions, without reasonable explanation, is out of line with any pattern that has previously emerged or may have been deliberately structured to avoid detection;\\n(d)\\ta customer's refusal to provide the information requested without reasonable explanation;\\n(e)\\twhere a customer who has just entered into a business relationship uses the relationship for a single Transaction or for only a very short period of time;\\n(f)\\textensive use of offshore accounts, companies or structures in circumstances where the customer's economic needs do not support such requirements;\\n(g)\\tunnecessary routing of funds through third-party accounts; or\\n(h)\\tunusual Transactions without an apparently profitable motive.\\n14.2.3.Guidance.2. CDD measures form the basis for recognising suspicious activity or Transactions. Sufficient guidance must therefore be given to the Relevant Person's Employees to enable them to form a suspicion or to recognise when they have reasonable grounds to suspect that money laundering or terrorist financing is taking place. This should involve training that will enable relevant Employees to seek and assess the information that is required for them to judge whether a Person is involved in suspicious activity or Transactions related to money laundering or terrorist financing.\\n14.2.3.Guidance.3. Where appropriate, a Relevant Person should also utilise the methods described in paragraph 1 above to detect a range of Financial Crimes, including fraud. Bearing in mind the evolving nature of Financial Crime and the methods used to further it, a Relevant Person should apply best practice when determining which behaviours would be considered suspicious and what measures are required to detect suspicious activity and Transactions. Such practices may include, but are not limited to, incorporating the analysis of customer behaviour metrics into the monitoring of suspicious activity and Transactions.\\n14.2.3.Guidance.4. The requirement for Employees to notify the Relevant Person's MLRO should include situations when no business relationship was developed because the circumstances were suspicious.\\n14.2.3.Guidance.5. A Relevant Person may allow its Employees to consult with their line managers before sending a report to the MLRO. The Regulator would expect that such consultation does not prevent making a report whenever an Employee has stated that he has knowledge, suspicion or reasonable grounds for knowing or suspecting that a Person may be involved in money laundering. Whether or not an Employee consults with his line manager or other Employees, the responsibility remains with the Employee to decide for himself whether a notification to the MLRO should be made.\\n14.2.3.Guidance.6. An Employee, including the MLRO, who considers that a Person has engaged in or is engaging in activity or Transactions that he knows or suspects to be suspicious would not be expected to know the exact nature of the criminal offence or that the particular funds were definitely those arising from the crime of money laundering or terrorist financing.\\n14.2.3.Guidance.7. Activity or Transactions that appear unusual are not necessarily suspicious. Even customers with a stable and predictable Transaction profile will have periodic Transactions that are unusual for them. Many customers will, for perfectly good reasons, have an erratic pattern of Transactions or account activity. So the unusual is, in the first instance, only a basis for further inquiry, which may in turn require judgement as to whether it is suspicious. A Transaction or activity may not be suspicious at the time, but if suspicions are raised later, an obligation to report it then arises.\\n14.2.3.Guidance.8. Effective CDD measures may provide the basis for recognising unusual and suspicious activity and Transactions. Refusal to provide documentation to support CDD or refusal to disclose a beneficial owner may be considered suspicious activity. Where there is a customer relationship, suspicious activity will often be one that is inconsistent with a customer's known legitimate activity, or with the normal business activities for that type of account or customer. Therefore, the key to recognising \\\"suspicious activity\\\" is knowing enough about the customer and the customer's normal expected activities to recognise when their activity is abnormal.\\n14.2.3.Guidance.9. A Relevant Person may consider implementing policies and procedures whereby disciplinary action is taken against an Employee who fails to notify the Relevant Person's MLRO.\\n14.2.3.Guidance.10. Relevant Persons should comply with guidance issued by the EOCN with regard to identifying and reporting suspicious activity and Transactions relating to money laundering, terrorist financing and proliferation financing.\\n14.3 Suspicious Activity/Transaction Reports\\n14.3.1 A Relevant Person must ensure that where the Relevant Person's MLRO receives an internal notification of suspicious activity under Rule \\u200e14.2.2, the MLRO, without delay:\\n(a)\\tinvestigates and documents the circumstances in relation to which the notification made under Rule \\u200e14.2.2 was made;\\n(b)\\tdetermines whether in accordance with Federal AML Legislation a SAR/STR must be made and documents such determination; and\\n(c)\\tif required, make a SAR/STR as soon as practicable.\\n14.3.2 The MLRO must, following receipt of an internal notification of suspicious activity under Rule \\u200e14.2.2, document:\\n(a)\\tthe steps taken to investigate the circumstances in relation to which the internal notification is made; and\\n(b)\\twhere no external SAR/STR is made, the reasons why no such report was made.\\n14.3.3 Where, following a notification to the MLRO of suspicious activity under \\u200e14.2.2, no SAR/STR is made, a Relevant Person must record the reasons for not making a SAR/STR.\\n14.3.4 A Relevant Person must ensure that if the MLRO decides to make a SAR/STR, his decision is made independently and is not subject to the consent or approval of any other Person.\\n14.3.5 Relevant Persons are required to register on goAML upon receipt of their Financial Services Permission, Recognition Order or registration licence in order to submit SAR/STRs.\\n14.3.5.Guidance\\n14.3.5.Guidance.1. Relevant Persons are reminded that the failure to report suspicions of money laundering or terrorist financing may constitute a criminal offence that is punishable under the laws of the U.A.E.\\n14.3.5.Guidance.2. Relevant Persons should comply with guidance issued by the EOCN regarding reporting suspicious activity and Transactions relating to money laundering, terrorist financing and proliferation financing.\\n14.3.5.Guidance.3. SARs/STRs under Federal AML Legislation should be submitted to the FIU via goAML. The dedicated mechanism for registering and reporting on goAML is available on the Regulator\\u2019s website. Failure to register on goAML may lead to the Regulator taking action.\\n14.3.5.Guidance.4. In the preparation of a SAR/STR, if a Relevant Person knows or assumes that the funds which form the subject of the report do not belong to a customer but to a third party, this fact and the details of the Relevant Person's proposed course of further action in relation to the case should be included in the report.\\n14.3.5.Guidance.5. If a Relevant Person has filed a SAR/STR, the FIU may instruct the Relevant Person on how to continue its business relationship, including effecting any Transaction with a Person. If the customer in question expresses his wish to move the funds before the Relevant Person receives instruction from the FIU on how to proceed, the Relevant Person should immediately contact the FIU for further instructions.\\n14.4 Suspension of Transactions and \\u201cno tipping-off\\u201d requirement\\n14.4.1 A Relevant Person must not carry out Transactions that it knows or suspects or has reasonable grounds for knowing or suspecting to be related to money laundering or terrorist financing until it has informed the FIU pursuant to Rule \\u200e14.3.1.\\n14.4.1.Guidance\\n14.4.1.Guidance.1. Relevant Persons are reminded that in accordance with Federal AML Legislation, Relevant Persons or any of their Employees must not tip off any Person, that is, inform any Person that he is being scrutinised, or investigated by any other competent authority, for possible involvement in suspicious Transactions or activity related to money laundering or terrorist financing.\\n14.4.1.Guidance.2. If a Relevant Person reasonably believes that performing CDD measures will tip off a customer or potential customer, it may choose not to pursue that process and should file a Suspicious Activity/Transaction Report. Relevant Persons should ensure that their Employees are aware of and sensitive to these issues when considering the CDD measures.\\n14.5 Record-keeping\\n14.5.1 All relevant details of any notification to the MLRO under Rule \\u200e14.2 or Suspicious Activity/Transaction Reports filed pursuant to Rule \\u200e14.3 must be kept for at least six years from the date on which the report was made.\\n14.6 Freezing of assets\\n14.6.Guidance \\nThe Regulator has certain powers under FSMR to impose a requirement restricting an Authorised Person or Recognised Body from disposing of or transferring property including, for example, assets or other funds suspected of relating to money laundering. It may also apply to the ADGM Courts for an order restraining a Person from transferring or disposing of any assets suspected of relating to money laundering or terrorist financing. In cases involving suspected money laundering or terrorist financing, the Regulator will usually take such action in coordination with the FIU.\\n\\n\"\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "# data_dir = \"bm25_noisy_oracle_passages_oracle_documents\" only for echr_qa\n",
    "data_dir = \"bm25_oracle_passages_oracle_documents\"\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "create = True\n",
    "def retrieve_top_passages(entry):\n",
    "    query = entry['gold_text']\n",
    "    all_passages = entry['oracle_documents_passages']\n",
    "    all_passages_text = [f\"{passage_arr[0]}\\n{passage_arr[1]}\" for passage_arr in all_passages]\n",
    "    corpus_tokens = bm25s.tokenize(all_passages_text, stopwords=\"en\", stemmer=stemmer)\n",
    "    retriever = bm25s.BM25()\n",
    "    retriever.index(corpus_tokens)\n",
    "    query_tokens = bm25s.tokenize(query, stemmer=stemmer)\n",
    "    local_top_k = top_k if top_k <= len(all_passages_text) else len(all_passages_text)\n",
    "    results, _ = retriever.retrieve(query_tokens, corpus=all_passages_text, k=local_top_k)\n",
    "    results = results.squeeze(0)\n",
    "    return {f\"top_{top_k}_passages\": results}  \n",
    "try:\n",
    "    if not create:\n",
    "        new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "except:\n",
    "    print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "    create = True\n",
    "\n",
    "if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "    new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "    new_dataset = new_dataset.map(retrieve_top_passages, num_proc=num_proc)\n",
    "    new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "else:\n",
    "    print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "print(json.dumps(new_dataset['test'][0][f\"top_{top_k}_passages\"][0], indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the BM25 Relevant Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] adding top_10_passages to OBLI_QA-generation-workshop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2183fea7ec114acc87f227091d634ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff615a71fc042609404013b09cb559c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/2786 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45db170250a54e0bad0e549ba887e2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93cfc7bd17264b5fbef398315a051cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc39b993274545f7ad2d6aef6b4d5d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2291c37320004411888124aea00aa1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d85b26000a41feb9b9d7e77864be53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f67645619cc42fe89c8f29a2b240914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7a2977a6434fa98473e86d66aaaa56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1265383e75664a29a2bb91c79c21148d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55de266fc3e44dab3d76fb347fd47fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b6e4c0827f48f08ae9f09dfd7e4ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c76dec06534c128143fefd191f30c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8cc3154abd4845bfae963adcd76bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/709 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1\\n9. AML/TFS COMPLIANCE AND THIRD PARTIES\\n9.1 Reliance on a third party\\n9.1.1\\n9.1.1.(1) A Relevant Person may rely on the following third parties to conduct one or more of the elements of CDD on its behalf:\\n(a)\\tan Authorised Person or Recognised Body;\\n(b)\\ta law firm, notary, or other independent legal business, accounting firm, audit firm or insolvency practitioner or an equivalent Person in another jurisdiction;\\n(c)\\ta Financial Institution;\\n(d)\\ta member of the Relevant Person's Group; or\\n(e)\\tother specialised utilities for the provision of outsourced AML/TFS services.\\n9.1.1.(2) In (1), a Relevant Person may rely on the information previously obtained by a third party which covers one or more elements of CDD.\\n9.1.1.(3) Where a Relevant Person seeks to rely on a Person in (1) it may only do so if and to the extent that:\\n(a)\\tit immediately obtains the necessary CDD information from the third party in (1);\\n(b)\\tit takes adequate steps to satisfy itself that certified copies of the documents used to undertake the relevant elements of CDD will be available from the third party on request without delay;\\n(c)\\tthe Person in (1)(b) to (d) is subject to regulation, including AML/TFS compliance requirements, by a Non-ADGM Financial Services Regulator or other competent authority in a country with AML/TFS regulations which are equivalent to the standards set out in the FATF Recommendations and it is supervised for compliance with such regulations;\\n(d)\\tthe Person in (1) has not relied on any exception from the requirement to conduct any relevant elements of CDD which the Relevant Person seeks to rely on; and\\n(e)\\tin relation to (2), the information is up to date.\\n9.1.1.(4) Where a Relevant Person relies on a member of its Group to conduct one or more of the elements of CDD on its behalf, such Group member need not meet the condition in (3)(c) if:\\n(a)\\tthe Group is subject to policies and requirements equivalent to FATF standards, either:\\n(i)\\twhere the Group applies and implements a Group-wide policy on CDD and record-keeping which is equivalent to the standards set by FATF; or\\n(ii)\\twhere the effective implementation of those CDD and record-keeping requirements and AML/TFS programmes are supervised at Group level by a Non-ADGM Financial Services Regulator or other competent authority in a jurisdiction with AML/TFS regulations that are equivalent to the standards set out in the FATF Recommendations;\\n(b)\\tno exception from identification obligations has been applied in the original identification process; and\\n(c)\\ta written statement is received from the introducing member of the Relevant Person's Group confirming that:\\n(i)\\tthe customer has been identified in accordance with the relevant standards under (4)(a) and (b);\\n(ii)\\tany identification evidence can be accessed by the Relevant Person without delay; and\\n(iii)\\tthe identification evidence will be kept for at least six years.\\n9.1.1.(5) If a Relevant Person is not reasonably satisfied that a customer or Beneficial Owners has been identified and verified by a third party in a manner consistent with these Rules, the Relevant Person must immediately perform the CDD itself with respect to any deficiencies identified.\\n(6)\\tNotwithstanding the Relevant Person's reliance on a Person in \\u200e9.1.1(1), the Relevant Person remains responsible for compliance with, and liable for any failure to meet the CDD requirements in the AML Rulebook.\\n9.1.2\\n9.1.2.(1) When assessing under Rule \\u200e9.1.1(3) or (4) if AML/TFS regulations in another jurisdiction are equivalent to FATF standards, a Relevant Person must take into account factors including, but not limited to:\\n(a)\\tmutual evaluations, assessment reports or follow-up reports published by FATF, the IMF, the World Bank, the OECD or other International Organisations;\\n(b)\\tmembership of FATF or other international or regional groups such as the MENAFATF or the Gulf Co-operation Council;\\n(c)\\tcontextual factors such as political stability or the level of corruption in the jurisdiction;\\n(d)\\tevidence of recent criticism of the jurisdiction, including in:\\n(i)\\tFATF advisory notices;\\n(ii)\\tpublic assessments of the jurisdiction\\u2019s AML/TFS regimes by organisations referred to in (a); or\\n(iii)\\treports by other relevant non-government organisations or specialist commercial organisations;\\n(e)\\twhether adequate arrangements exist for co-operation between the AML/TFS regulator in that jurisdiction and the Regulator.\\n9.1.2.(2) A Relevant Person making an assessment under (1) must rely only on sources of information that are reliable and up to date.\\n9.1.2.(3) A Relevant Person must keep adequate records of how it made its assessment, including the sources and materials considered.\\n9.1.2.Guidance\\n9.1.2.Guidance.1. In complying with Rule \\u200e\\u200e9.1.1(3)(a), \\\"immediately obtaining the necessary CDD information\\\" means obtaining all relevant CDD information, and not just basic information such as name and address. However, compliance can be achieved by having the information sent in an email or other appropriate means. For the avoidance of doubt, it does not necessarily require a Relevant Person to immediately obtain the underlying certified documents used by the third party to undertake its CDD because under Rule \\u200e9.1.1(3)(b), these need only be available on request without delay.\\n9.1.2.Guidance.2. The Regulator would expect a Relevant Person, in complying with Rule \\u200e\\u200e9.1.1(5), to fill any gaps in the CDD process as soon as it becomes aware that a customer or Beneficial Owners has not been identified and verified by the third party in a manner consistent with these Rules.\\n9.1.2.Guidance.3. If a Relevant Person acquires another business, either in whole or in substantial part, the Regulator would permit the Relevant Person to rely on the CDD conducted by the business it is acquiring, but would expect the Relevant Person to have done the following:\\n(a)\\tas part of its due diligence for the acquisition, to have taken a reasonable sample of the prospective customers to assess the quality of the CDD undertaken; and\\n(b)\\tto have undertaken CDD on all the customers to cover any deficiencies identified in (a) as soon as possible following the acquisition, prioritising high-risk customers.\\n9.1.2.Guidance.4. Where the legislative framework of a jurisdiction (such as secrecy or data protection legislation) prevents a Relevant Person from having access to CDD information upon request without delay as referred to in Rule \\u200e9.1.1(3)(b), the Relevant Person should undertake the relevant CDD itself and should not seek to rely on the relevant third party.\\n9.1.2.Guidance.5. If a Relevant Person relies on a third party located in a foreign jurisdiction to conduct one or more elements of CDD on its behalf, the Relevant Person must ensure that the foreign jurisdiction has AML/TFS regulations which are equivalent to the standards in the FATF Recommendations (see Rule \\u200e9.1.1(3)(c)).\\n9.1.2.Guidance.6. Relevant Persons should follow directives issued by the NAMLCFTC. For example, Relevant Persons are prohibited from using third parties located in Jurisdictions Subject to a Call for Action to perform CDD.\\n9.2 Business partner identification\\n9.2.1\\n9.2.1.(1) Prior to establishing the business relationship, a Relevant Person must establish and verify the identity of its business partners by obtaining sufficient and satisfactory evidence of the identity of any business partner it relies upon in carrying on its Regulated Activities.\\n(a)\\tA Relevant Person must maintain accurate and up-to-date information and conduct ongoing due diligence on its business partners, throughout the course of the business relationship.\\n(b)\\tIf at any time a Relevant Person becomes aware that it lacks sufficient information or documentation concerning a business partner's identification, or develops a concern about the accuracy of its current information or documentation, it must promptly obtain appropriate material to verify such business partner's identity.\\n9.2.1.(2) In the context of this Rule, a 'business partner' includes:\\n(a)\\ta third party as specified in Rule \\u200e9.1.1(1);\\n(b)\\ta member of the Relevant Person's Group;\\n(c)\\ta Correspondent Bank; or\\n(d)\\tany other service provider.\\n9.2.1.(3) A Relevant Person that establishes, operates or maintains a Correspondent Account for a Correspondent Banking Client must ensure that it has arrangements to:\\n(a)\\tconduct due diligence in respect of the opening of a Correspondent Account for a Correspondent Banking Client, including measures to identify:\\n(i)\\tits ownership and management structure;\\n(ii)\\tits major business activities and customer base;\\n(iii)\\tits location; and\\n(iv)\\tthe intended purpose of the Correspondent Account;\\n(b)\\tidentify all third parties that will use the Correspondent Account; and\\n(c)\\tmonitor Transactions processed through a Correspondent Account that has been opened by a Correspondent Banking Client, in order to detect and report any suspicion of money laundering.\\n9.2.1.Guidance \\nUnder (2)(d), service providers include agents that directly facilitate the activities of Authorised Persons in servicing their clients, as distinct from other service providers that provide purely ancillary services, such as IT, facilities management etc. to an Authorised Person.\\n9.2.2 A Relevant Person must not:\\n9.2.2.(1) establish a correspondent banking relationship with a Shell Bank;\\n9.2.2.(2) establish or keep anonymous accounts or accounts in false names; or\\n9.2.2.(3) maintain a nominee account which is held in the name of one Person, but controlled by or held for the benefit of another Person whose identity has not been disclosed to the Relevant Person.\\n9.2.2.Guidance\\n9.2.2.Guidance.1. \\\"Know your business partner\\\" is as important as \\\"Know Your Customer\\\". A Relevant Person is therefore required to verify the identity of a prospective business partner and to obtain evidence of it. The same documentation that is used to identify customers should be obtained from the business partner prior to conducting any business.\\n9.2.2.Guidance.2. A Relevant Person should verify whether any secrecy or data protection law exists in the country of incorporation of the business partner that would prevent access to relevant data.\\n9.2.2.Guidance.3. The requirement to identify the business partner is meant to cover only those business partners who may pose any relevant money laundering risks to the Relevant Person. Hence, a Relevant Person would not be required to establish and verify the identity of, for example, its maintenance or cleaning service.\\n9.2.2.Guidance.4. The Regulator may take into account the identity of a Relevant Person's business partner and the nature of their relationship in considering the fitness and propriety of a Relevant Person.\\n9.2.2.Guidance.5. Before entering into a business relationship, a Relevant Person should conduct a due diligence investigation, which includes ensuring that the business partner is an existing Person authorised to conduct the kind of business in question and, if applicable, verifying that this Person is duly regulated by a Financial Services Regulator or other relevant regulatory authority or regulator. In accordance with \\\"The Wolfsberg Anti-Money Laundering Principles for Correspondent Banking\\\", the Relevant Person should take into account, and verify the nature of:\\n(a)\\tthe business to be conducted and the major business activities of the business partner;\\n(b)\\tthe jurisdiction where the business partner is located as well as that of its parent; and\\n(c)\\tthe transparency and the nature of the ownership and the management structure.\\n9.2.2.Guidance.6. A Relevant Person may also gather information about the reputation of the business partner, including whether it has been subject to investigation or regulatory action in relation to money laundering.\\n9.2.2.Guidance.7. A Relevant Person should adopt a risk-based approach when verifying its business partners' identities. Depending on the money laundering risks assessment of the Relevant Person's business partner, the Relevant Person should decide the level of detail of the business partner identification and verification process.\\n9.2.2.Guidance.8. A Relevant Person should have in place specific arrangements to ensure that adequate due diligence and identification measures with regard to the business relationship are taken.\\n9.2.2.Guidance.9. The Relevant Person should conduct regular reviews of the relationship with its business partners.\\n9.2.2.Guidance.10. The Senior Management or Governing Body of a Relevant Person should give its approval before it establishes any new correspondent banking relationships.\\n9.2.2.Guidance.11. A Relevant Person should also have arrangements to guard against establishing a business relationship with business partners who permit their accounts to be used by Shell Banks; further details on the definition of Shell Banks are set out in Guidance 2 to Rule \\u200e10.2.2.\\n9.3 Outsourcing and agents\\n9.3.1 A Relevant Person which outsources any one or more elements of its CDD to a service provider (including those within its Group) remains responsible for compliance with, and liable for any failure to meet, such obligations.\\n9.3.1A Prior to appointing a service provider to undertake CDD, a Relevant Person must undertake an initial assurance assessment to evaluate the suitability of the service provider and must ensure that the service provider's obligations are clearly documented in a binding agreement.\\n9.3.1B After engaging a service provider the Relevant Person must undertake periodic assurance assessments to ensure that the services provided meet the obligations recorded in the binding agreement and allow it to meet all the requirements that it is subject to.\\n\\n9.3.1B.Guidance\\n9.3.1B.Guidance.1. The use by a Relevant Person of a service provider\\u2019s eKYC System that enables a Relevant Person to undertake eKYC constitutes outsourcing for the purposes of Rule \\u200e9.3.1.\\n9.3.1B.Guidance.2. When undertaking an assurance assessment of an eKYC System for the purpose of Rule \\u200e9.3.1A, a Relevant Person should seek to establish that the eKYC System is reliable and independent, and allows the Relevant Person to comply with all applicable Rules of the Regulator. In addition, a Relevant Person should consider applying guidance on assurance standards issued by the Regulator, competent U.A.E. authorities, FATF, and other relevant standard setting bodies.\\n9.3.1B.Guidance.3. In limited circumstances, a Relevant Person may place reliance on the assurance assessment of the eKYC System conducted entirely by another entity. Such circumstances comprise the following.\\n(a)\\tWhere an assurance assessment of the eKYC System has been undertaken by a Related entity and specifically addresses the Rules and Regulations applicable to the Relevant Person. In such circumstances, the Relevant Person remains responsible for the eKYC System\\u2019s compliance with applicable Rules and Laws and it should maintain a copy of the assessment.\\n(b)\\tWhere the eKYC System has been authorised by a competent authority of the U.A.E. or a competent authority in a jurisdiction with AML/TFS laws equivalent to the U.A.E. In such circumstances, the eKYC system should be authorised for use in CDD. Further, the Relevant Person should undertake its own review to ensure that any use of the relevant eKYC System is appropriate and enables compliance with all Rules and Regulations applicable to the Relevant Person.\\n(c)\\tWhere a Relevant Person chooses to employ a third party to assist in its own assurance assessment of the eKYC System, it should ensure that a competent and independent firm with relevant expertise and resources be employed. The Relevant Person remains wholly responsible for the eKYC System\\u2019s compliance with, and any failure to meet, the Rules and Regulations applicable to the Relevant Person.\\n9.3.1B.Guidance.4. In complying with Rule \\u200e9.3.1, a Relevant Person should ensure that the service provider can be replaced with minimal disruption in the event the outsourcing arrangement is terminated.\\n9.3.1B.Guidance.5. An Authorised Person is also required to comply with the outsourcing obligations in GEN 3.3.31 and 3.3.32 and PRU 6.8. A Recognised Body is also required to comply with the outsourcing obligations in MIR 2.14.\\n9.3.2 Authorised Persons Providing Money Services\\n9.3.2.(1) An Authorised Person that is engaged in Providing Money Services must:\\n(b)\\tmaintain a complete, current and accurate register of all agents and members of its Group it uses to conduct its operations and make that register available to the Regulator upon request;\\n(c)\\tinclude all agents and members of its Group identified in (a) as part of its AML/TFS compliance programme and monitor the compliance of such agents and members of its Group with the requirements of its AML/TFS programme;\\n(d)\\tcomply with all AML/TFS requirements imposed in all jurisdictions within which it operates and ensure the compliance of its agents and members of its Group operating on its behalf with all AML/TFS requirements in the jurisdictions in which they are operating;\\n(e)\\twhen executing a Payment Transaction, assess and consider all relevant information, including information about the Payer and the Payee, including any beneficiary as may be applicable, and require its agents and members of its Group, as appropriate, to determine whether a Suspicious Activity/Transaction Report should be filed by it or its agents or a member of its Group; and\\n(f)\\twhere appropriate, ensure that the relevant equivalent of a Suspicious Activity/Transaction Report is filed in all other jurisdictions related to a suspicious Payment Transaction and make available to all authorities responsible for AML/TFS compliance all transaction information related to the suspicious transaction.\\n9.3.2.(2) An Authorised Person making an assessment under (1) must rely upon current sources of information when making such assessment and must keep adequate records concerning such assessments, including all sources and materials considered, for a period of at least six years.\\n9.3.2.Guidance \\nAgents directly facilitate the activities of Authorised Persons in servicing their clients, as distinct from other service providers that provide purely ancillary services, such as IT, facilities management etc. to an Authorised Person.\\n\\n\"\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "data_dir = \"bm25_relevant_passages_oracle_documents\"\n",
    "\n",
    "create = True\n",
    "def retrieve_top_passages(entry):\n",
    "    query = entry['previous_text']\n",
    "    all_passages = entry['oracle_documents_passages']\n",
    "    all_passages_text = [f\"{passage_arr[0]}\\n{passage_arr[1]}\" for passage_arr in all_passages]\n",
    "    corpus_tokens = bm25s.tokenize(all_passages_text, stopwords=\"en\", stemmer=stemmer)\n",
    "    retriever = bm25s.BM25()\n",
    "    retriever.index(corpus_tokens)\n",
    "    query_tokens = bm25s.tokenize(query, stemmer=stemmer)\n",
    "    local_top_k = top_k if top_k <= len(all_passages_text) else len(all_passages_text)\n",
    "\n",
    "    results, _ = retriever.retrieve(query_tokens, corpus=all_passages_text, k=local_top_k)\n",
    "    results = results.squeeze(0)\n",
    "    return {f\"top_{top_k}_passages\": results}  \n",
    "\n",
    "try:\n",
    "    if not create:\n",
    "        new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "except:\n",
    "    print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "    create = True\n",
    "    \n",
    "if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "    new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "    new_dataset = new_dataset.map(retrieve_top_passages, num_proc=num_proc)\n",
    "    new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "else:\n",
    "    print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "print(json.dumps(new_dataset['test'][0][f\"top_{top_k}_passages\"][0], indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dense Oracle Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dense Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "encoder_name = \"jhu-clsp/LegalBERT-DPR-CLERC-ft\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n",
    "model = AutoModel.from_pretrained(encoder_name)\n",
    "# model = torch.compile(model)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "clean_encoder_name = encoder_name.replace(\"/\", \"_\")\n",
    "data_dir = \"dense_oracle_passages_oracle_documents\"\n",
    "data_dir = f\"{data_dir}/{clean_encoder_name}\"\n",
    "\n",
    "def normalize_embeddings(embeddings):\n",
    "    return embeddings / torch.norm(embeddings, p=2, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "def embed_texts(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=model.config.max_position_embeddings).to(device)\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        token_embeddings = model(**inputs).last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        embeddings = sum_embeddings / sum_mask\n",
    "    embeddings = normalize_embeddings(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def retrieve_top_passages_batch(entries):\n",
    "    all_queries = entries['gold_text']\n",
    "    all_passages_batch = entries['oracle_documents_passages']\n",
    "    all_passages_text = [passage[1] for passages in all_passages_batch for passage in passages]\n",
    "    query_embeddings = embed_texts(all_queries, tokenizer, model)  # Shape: (batch_size, dim)\n",
    "    passage_embeddings = embed_texts(all_passages_text, tokenizer, model)  # Shape: (total_passages, dim)\n",
    "    dim = passage_embeddings.size(1)\n",
    "    # index = faiss.IndexFlatL2(dim)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(passage_embeddings.cpu().numpy())  # FAISS requires NumPy arrays here\n",
    "    distances, indices = index.search(query_embeddings.cpu().numpy(), top_k)\n",
    "    top_passages_batch = [[all_passages_text[index] for index in index_arr] for index_arr in indices]\n",
    "    return {f\"top_{top_k}_passages\": top_passages_batch}\n",
    "\n",
    "try:\n",
    "    new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "except:\n",
    "    print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "    create = True\n",
    "    \n",
    "\n",
    "if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "    new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "    new_dataset = new_dataset.map(\n",
    "        lambda batch: retrieve_top_passages_batch(batch),\n",
    "        batched=True,\n",
    "        batch_size=4\n",
    "    )\n",
    "    new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "else:\n",
    "    print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "print(json.dumps(new_dataset['train'][0][f\"top_{top_k}_passages\"][0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_bm25_oracle_passages_oracle_documents = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"bm25_oracle_passages_oracle_documents\")\n",
    "print(\"===================================\")\n",
    "print(dataset_bm25_oracle_passages_oracle_documents['train'][0]['gold_text'])\n",
    "print(dataset_bm25_oracle_passages_oracle_documents['train'][0]['top_10_passages'][0])\n",
    "\n",
    "dataset_bm25_relevant_passages_oracle_documents = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"bm25_relevant_passages_oracle_documents\")\n",
    "print(\"===================================\")\n",
    "print(dataset_bm25_relevant_passages_oracle_documents['train'][0]['previous_text'])\n",
    "print(dataset_bm25_relevant_passages_oracle_documents['train'][0]['top_10_passages'][0])\n",
    "\n",
    "dataset_dense_oracle_passages_oracle_documents = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"dense_oracle_passages_oracle_documents\")\n",
    "print(\"===================================\")\n",
    "print(dataset_dense_oracle_passages_oracle_documents['train'][0]['gold_text'])\n",
    "print(dataset_dense_oracle_passages_oracle_documents['train'][0]['top_10_passages'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# import faiss\n",
    "# import numpy as np\n",
    "\n",
    "# encoder_name = \"jhu-clsp/LegalBERT-DPR-CLERC-ft\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n",
    "# model = AutoModel.from_pretrained(encoder_name)\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# top_k = 10\n",
    "# data_dir = \"dense_oracle_passages_oracle_documents\"\n",
    "\n",
    "# def embed_passages(passages, tokenizer, model):\n",
    "#     inputs = tokenizer(passages, return_tensors=\"pt\", padding=True, truncation=True, max_length=model.config.max_position_embeddings).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         embeddings = model(**inputs).pooler_output\n",
    "#     return embeddings.cpu().numpy()\n",
    "\n",
    "# def retrieve_top_passages(entry):\n",
    "#     query = entry['gold_text']\n",
    "#     all_passages = entry['oracle_documents_passages']\n",
    "#     all_passages_text = [f\"{passage_arr[0]}\\n{passage_arr[1]}\" for passage_arr in all_passages]\n",
    "    \n",
    "#     passage_embeddings = embed_passages(all_passages_text, tokenizer, model)\n",
    "#     query_embedding = embed_passages([query], tokenizer, model).squeeze(0)\n",
    "\n",
    "#     dim = passage_embeddings.shape[1]\n",
    "#     index = faiss.IndexFlatL2(dim)\n",
    "#     index.add(passage_embeddings)\n",
    "    \n",
    "#     # Retrieve top-k similar passages\n",
    "#     distances, indices = index.search(np.expand_dims(query_embedding, axis=0), top_k)\n",
    "#     top_passages = [all_passages_text[idx] for idx in indices[0]]\n",
    "    \n",
    "#     return {f\"top_{top_k}_passages\": top_passages}\n",
    "\n",
    "# try:\n",
    "#     workshop_hf_name = f\"CLERC-generation-workshop\"\n",
    "#     new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "# except:\n",
    "#     print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "#     create = True\n",
    "    \n",
    "# if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "#     print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "#     new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "#     new_dataset = new_dataset.map(retrieve_top_passages, num_proc=num_proc)\n",
    "#     new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "# else:\n",
    "#     print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "# print(json.dumps(new_dataset['train'][0][f\"top_{top_k}_passages\"][0], indent=4)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
