{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "service_account_json_file = os.path.join('../../', 'thesis_service_account.json')\n",
    "if not os.path.exists(service_account_json_file):\n",
    "    raise FileNotFoundError(f\"[!] service account JSON file not found: {service_account_json_file}\")\n",
    "\n",
    "\n",
    "datasets = [\n",
    "    \"clerc\",\n",
    "    \"cuad\",\n",
    "    \"echr_qa\",\n",
    "    \"oal_qa\",\n",
    "    \"obli_qa\",\n",
    "]\n",
    "\n",
    "def compute_length_percentiles(hf_dataset, split_name=\"test\", column_name=\"citations\"):\n",
    "    lengths = []\n",
    "    list_lengths = []\n",
    "    # Iterate over rows in the chosen split (e.g., 'train', 'test')\n",
    "    for example in hf_dataset[split_name]:\n",
    "        if isinstance(example[column_name], str):\n",
    "            word_count = len(example[column_name].split())\n",
    "            lengths.append(word_count)\n",
    "            list_lengths.append(1)\n",
    "        elif isinstance(example[column_name], list):\n",
    "            list_lengths.append(len(example[column_name]))\n",
    "            for nested_arr in example[column_name]:\n",
    "                if len(nested_arr) > 1:\n",
    "                    # The second item is the \"document\" text\n",
    "                    doc_text = nested_arr[1]\n",
    "                    # Split by whitespace to count words\n",
    "                    word_count = len(doc_text.split())\n",
    "                    lengths.append(word_count)\n",
    "\n",
    "    if not lengths:\n",
    "        print(f\"[!] No documents found in split={split_name}, column={column_name}.\")\n",
    "        return None\n",
    "\n",
    "    lengths = np.array(lengths)\n",
    "\n",
    "    stats = {\n",
    "        \"mean\": float(np.mean(lengths)),\n",
    "        \"p25\": float(np.percentile(lengths, 25)),\n",
    "        \"p50\": float(np.percentile(lengths, 50)),\n",
    "        \"p75\": float(np.percentile(lengths, 75)),\n",
    "        \"p90\": float(np.percentile(lengths, 90)),\n",
    "        \"p95\": float(np.percentile(lengths, 95)),\n",
    "        \"p99\": float(np.percentile(lengths, 99)),\n",
    "        \"count\": np.mean(list_lengths)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "\n",
    "def add_record(dataset, data):\n",
    "    \"\"\"\n",
    "    Example function that appends a row to a spreadsheet.\n",
    "    The logic is partially from your code snippet, with small adjustments.\n",
    "    \"\"\"\n",
    "    new_row = [dataset]\n",
    "    sheet_columns_data = [\n",
    "        \"citations.mean\", \"citations.p25\", \"citations.p50\", \"citations.p75\", \"citations.p99\", \"citations.count\",\n",
    "        \"gold_text.mean\", \"gold_text.p25\", \"gold_text.p50\", \"gold_text.p75\", \"gold_text.p99\",\n",
    "        \"count\"\n",
    "    ]\n",
    "    for key in sheet_columns_data:\n",
    "        keys = key.split('.')\n",
    "        value = data\n",
    "        for k in keys:\n",
    "            if value is None:\n",
    "                break\n",
    "            value = value.get(k, None)\n",
    "            if value is None:\n",
    "                break\n",
    "        new_row.append(value)\n",
    "\n",
    "    new_row[0] = new_row[0].upper()  # e.g., OAL_QA\n",
    "\n",
    "    scope = [\n",
    "        \"https://spreadsheets.google.com/feeds\",\n",
    "        \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "        \"https://www.googleapis.com/auth/drive.file\",\n",
    "        \"https://www.googleapis.com/auth/drive\"\n",
    "    ]\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(service_account_json_file, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1bE5AbY1hrqlR-_v-ohLCgHA6hFvRCTpH4lrRPqXm9UU/edit?usp=sharing\"\n",
    "    sheet = client.open_by_url(spreadsheet_url)\n",
    "    worksheet = sheet.worksheet(f\"Dataset â€” Lengths\")\n",
    "\n",
    "    worksheet.append_row(new_row, value_input_option=\"USER_ENTERED\")\n",
    "    # print(\"[!] added record to Google Sheets\")\n",
    "    \n",
    "for dataset in tqdm(datasets):\n",
    "    if dataset == \"clerc\":\n",
    "        workshop_hf_name = f\"CLERC-generation-workshop\"\n",
    "    elif dataset == \"echr_qa\":\n",
    "        workshop_hf_name = f\"ECHR_QA-generation-workshop\"\n",
    "    elif dataset == \"obli_qa\":\n",
    "        workshop_hf_name = f\"OBLI_QA-generation-workshop\"\n",
    "    elif dataset == \"cuad\":\n",
    "        workshop_hf_name = f\"CUAD-generation-workshop\"\n",
    "    elif dataset == \"oal_qa\":\n",
    "        workshop_hf_name = f\"OAL_QA-generation-workshop\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid dataset {dataset}\")\n",
    "    current_chosen_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"data\")\n",
    "    split_name = \"test\"\n",
    "    # print(\"[+] Loaded dataset:\", current_chosen_dataset)\n",
    "    citations_percentiles = compute_length_percentiles(current_chosen_dataset, split_name=split_name, column_name=\"citations\")\n",
    "    gold_text_percentiles = compute_length_percentiles(current_chosen_dataset, split_name=split_name, column_name=\"gold_text\")\n",
    "    data_to_publish = {\n",
    "        \"mean\": citations_percentiles[\"mean\"] if citations_percentiles else None,\n",
    "        \"citations\":{\n",
    "            \"mean\": citations_percentiles[\"mean\"] if citations_percentiles else None,\n",
    "            \"p25\": citations_percentiles[\"p25\"] if citations_percentiles else None,\n",
    "            \"p50\": citations_percentiles[\"p50\"] if citations_percentiles else None,\n",
    "            \"p75\": citations_percentiles[\"p75\"] if citations_percentiles else None,\n",
    "            \"p99\": citations_percentiles[\"p99\"] if citations_percentiles else None,\n",
    "            \"count\" : citations_percentiles[\"count\"] if citations_percentiles else None\n",
    "        },\n",
    "        \"gold_text\":{\n",
    "            \"mean\": gold_text_percentiles[\"mean\"] if gold_text_percentiles else None,\n",
    "            \"p25\": gold_text_percentiles[\"p25\"] if gold_text_percentiles else None,\n",
    "            \"p50\": gold_text_percentiles[\"p50\"] if gold_text_percentiles else None,\n",
    "            \"p75\": gold_text_percentiles[\"p75\"] if gold_text_percentiles else None,\n",
    "            \"p99\": gold_text_percentiles[\"p99\"] if gold_text_percentiles else None,\n",
    "        },\n",
    "        \"count\": len(current_chosen_dataset[split_name]),\n",
    "    }\n",
    "    add_record(dataset, data_to_publish)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
