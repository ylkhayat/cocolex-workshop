{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import argparse\n",
    "import bm25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 300\n",
    "chunk_overlap = 0.5\n",
    "\n",
    "num_proc = os.cpu_count() - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ed3877ba35427e8c652c2120f328d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e37940d3f248a083cd66a984b29367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d13862202914867a2486d93534aa498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbbd6c0856549f1a3dfd789268a075f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058da222c0b54544aa990998a87c521e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0271169a5db042728d969858dc68ada3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/751 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ylkhayat/ECHR_QA-generation-workshop/commit/4c81932985d7f357ba45eb092e798351b8bed965', commit_message='Upload dataset', commit_description='', oid='4c81932985d7f357ba45eb092e798351b8bed965', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ylkhayat/ECHR_QA-generation-workshop', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ylkhayat/ECHR_QA-generation-workshop'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workshop_hf_name = \"ECHR_QA-generation-workshop\"\n",
    "data_dir=\"data\"\n",
    "url=f\"https://huggingface.co/datasets/ylkhayat/ECHR_QA-generation-workshop/resolve/main/{data_dir}/\"\n",
    "\n",
    "current_chosen_dataset = load_dataset(\"parquet\", data_files={\"test\": [f\"{url}test-00000-of-00002.parquet\", f\"{url}test-00001-of-00002.parquet\"]})\n",
    "current_chosen_dataset = current_chosen_dataset.map(lambda record: {\"gold_text_with_citations\": record[\"gold_text\"]})\n",
    "current_chosen_dataset = current_chosen_dataset.map(lambda record: {\"gold_text\": record[\"gold_text_no_citations\"]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4606e27815794aeca84cfa8bc317e614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b168fe2d014748b783c4dc2475c2ec40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad43b79ec69b48f383c9e59cb6107666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46337810b41477c8e3e102d0645fb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/671 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ylkhayat/ECHR_QA-generation-workshop/commit/1d2418a4dbcdafd0d142f067eb973979f3781fca', commit_message='Upload dataset', commit_description='', oid='1d2418a4dbcdafd0d142f067eb973979f3781fca', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ylkhayat/ECHR_QA-generation-workshop', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ylkhayat/ECHR_QA-generation-workshop'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_chosen_dataset = current_chosen_dataset.remove_columns([\"gold_text_no_citations\"])\n",
    "current_chosen_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['docid', 'previous_text', 'gold_text', 'citations', 'oracle_documents_passages', 'oracle_documents_oracle_passages', 'gold_text_with_citations'],\n",
       "        num_rows: 1116\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_chosen_dataset = current_chosen_dataset.select_columns([\"docid\", \"previous_text\", \"gold_text\", \"gold_text_with_citations\", \"citations\", \"oracle_documents_passages\", \"top_10_passages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230d85a8e8374591a6e20cc83a4c8e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a008a7175f2e4d4e953fc3e69401c7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bef528a5e04dc6a01c2acf1f3a9769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd54c8e93a3945b6b1dbe1a862ffa3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ylkhayat/ECHR_QA-generation-workshop/commit/0f567c06117c60bac6534b0735c74c30b947647e', commit_message='Upload dataset', commit_description='', oid='0f567c06117c60bac6534b0735c74c30b947647e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ylkhayat/ECHR_QA-generation-workshop', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ylkhayat/ECHR_QA-generation-workshop'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workshop_hf_name = \"ECHR_QA-generation-workshop\"\n",
    "\n",
    "current_chosen_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"bm25_oracle_passages_oracle_documents\")\n",
    "new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"bm25_noisy_oracle_passages_oracle_documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab122713ef634f058b2c132bbf5c81d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/704 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2e0c921c5a44318ce0ff1c683924e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/147M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedbb2f9c95141568dd1c356ca675513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/152M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e818cf84c5e6437d82d6ca2ac13cd506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10acbce82ff040f0a34fe097971c3e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6656e46269c43909c0a693328d7a120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd13e82e8ce430c9c9e087435e47189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df6c3dc56c744e5989351c26182fc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11cf03ae1dd45aca49d69580029c792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/704 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ylkhayat/ECHR_QA-generation-workshop/commit/2cc0ff7d569ed9825ff0b0f099df1fad326f4a88', commit_message='Upload dataset', commit_description='', oid='2cc0ff7d569ed9825ff0b0f099df1fad326f4a88', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ylkhayat/ECHR_QA-generation-workshop', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ylkhayat/ECHR_QA-generation-workshop'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = \"echr_qa\"\n",
    "workshop_hf_name = \"ECHR_QA-generation-workshop\"\n",
    "\n",
    "current_chosen_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"bm25_oracle_passages_oracle_documents\")\n",
    "current_chosen_dataset = current_chosen_dataset.map(lambda record: {\"top_10_passages\": ['\\n'.join(oracle_passage_parts) for oracle_passage_parts in record[\"oracle_documents_oracle_passages\"]]})\n",
    "current_chosen_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"bm25_oracle_passages_oracle_documents\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] adding oracle_documents_passages to ECHR_QA-generation-workshop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ad346b805e43f5849fe462452a648c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943b83845366424a907bc8f10ac190f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5148ea938eee418c9c9165a77a276034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c74c296a24542afae587a88026f3226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b47032cf1734ca9b6d789a4d580bf75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47017ffcbf3545b8b9b8f10cfda04f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f985837fa8b4520a110484b5a0cee81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f708afdf1b67477e8f11503f79d91e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a247a57e1e3145669249f6e78b43363f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ylkhayat/ECHR_QA-generation-workshop/commit/b7be662d71666090f46b06c362be4597fbb7306f', commit_message='Upload dataset', commit_description='', oid='b7be662d71666090f46b06c362be4597fbb7306f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ylkhayat/ECHR_QA-generation-workshop', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ylkhayat/ECHR_QA-generation-workshop'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_document_into_passages(document_id: str, text: str, max_len: int, overlap: float):\n",
    "    if max_len <= 0:\n",
    "        raise ValueError(\"max_len must be a positive integer\")\n",
    "    if not (0 <= overlap < 1):\n",
    "        raise ValueError(\"overlap must be between 0 (inclusive) and 1 (exclusive)\")\n",
    "\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    step = int(max_len * (1 - overlap))\n",
    "    for i in range(0, len(words), step):\n",
    "        chunk_words = words[i:i+max_len]\n",
    "        if chunk_words:\n",
    "            chunk_text = ' '.join(chunk_words)\n",
    "            chunks.append([document_id, chunk_text])\n",
    "    return chunks\n",
    "\n",
    "def chunk_citations(record):\n",
    "    chunks = []\n",
    "    for citation_id, citation in record['citations']:\n",
    "        chunks.extend(chunk_document_into_passages(citation_id, citation, chunk_size, chunk_overlap))\n",
    "    return chunks\n",
    "\n",
    "if 'oracle_documents_passages' not in current_chosen_dataset.column_names['train'] or 'oracle_documents_passages' not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding oracle_documents_passages to {workshop_hf_name}\")\n",
    "    current_chosen_dataset = current_chosen_dataset.map(lambda record: {'oracle_documents_passages': chunk_citations(record)}, num_proc=num_proc)\n",
    "else:\n",
    "    print(f\"[!] oracle_documents_passages already exists in {workshop_hf_name}\")\n",
    "# current_chosen_dataset = current_chosen_dataset.select_columns([key_field, 'previous_text', 'gold_text', 'citations' , 'oracle_documents_passages'])\n",
    "current_chosen_dataset = current_chosen_dataset.select_columns([key_field, 'previous_text', 'gold_text', 'citations' , 'oracle_documents_passages', 'oracle_documents_oracle_passages'])\n",
    "current_chosen_dataset.push_to_hub(workshop_hf_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the BM25 Oracle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "import Stemmer \n",
    "stemmer = Stemmer.Stemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872d75c0571f4ddd9441fddc274c4aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8952b79135c74d74851c9d40e859732c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/61.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ff9b13bba14cb78668f056adceac59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/147M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfc61fa94454431b7b98d7862952f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/152M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bde59f1135474b89c0cd800f31d741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read file '/srv/elkhyo/.cache/huggingface/datasets/downloads/7d2ae8e096ae9fd1d8f6c42a730254c433122ef5a6ad6e65c7fc1434b7c26b70' with error <class 'datasets.table.CastError'>: Couldn't cast\n",
      "docid: int64\n",
      "previous_text: string\n",
      "gold_text: string\n",
      "citations: list<element: list<element: string>>\n",
      "  child 0, element: list<element: string>\n",
      "      child 0, element: string\n",
      "oracle_documents_passages: list<element: list<element: string>>\n",
      "  child 0, element: list<element: string>\n",
      "      child 0, element: string\n",
      "oracle_documents_oracle_passages: list<element: list<element: string>>\n",
      "  child 0, element: list<element: string>\n",
      "      child 0, element: string\n",
      "top_10_passages: list<element: string>\n",
      "  child 0, element: string\n",
      "-- schema metadata --\n",
      "huggingface: '{\"info\": {\"features\": {\"docid\": {\"dtype\": \"int64\", \"_type\":' + 611\n",
      "to\n",
      "{'docid': Value(dtype='int64', id=None), 'previous_text': Value(dtype='string', id=None), 'gold_text': Value(dtype='string', id=None), 'gold_text_no_citations': Value(dtype='string', id=None), 'citations': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'oracle_documents_passages': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'oracle_documents_oracle_passages': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'top_10_passages': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n",
      "because column names don't match\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] ECHR_QA-generation-workshop not found in bm25_oracle_passages_oracle_documents\n",
      "[*] adding top_10_passages to ECHR_QA-generation-workshop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c846265ee8f0424487fc0450a267c4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/1116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c489b18f51274c09a902b1c55c83c5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0225b6b290e549638a946a046b24431f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f017620003dd43b5ac02d1148d1f482b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24738a78b6ee43d3ac25e4b4ac996c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"001-45580\\n77. Article 8 (Art. 8) of the Convention provides as follows: \\\"1. Everyone has the right to respect for his private and family life, his home and his correspondence. 2. There shall be no interference by a public authority with the exercise of this right except such as is in accordance with the law and is necessary in a democratic society in the interests of national security, public safety or the economic well-being of the country, for the prevention of disorder or crime, for the protection of health or morals, or for the protection of the rights and freedoms of others.\\\"\"\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "data_dir = \"bm25_oracle_passages_oracle_documents\"\n",
    "\n",
    "\n",
    "create = False\n",
    "def retrieve_top_passages(entry):\n",
    "    query = entry['gold_text']\n",
    "    all_passages = entry['oracle_documents_passages']\n",
    "    all_passages_text = [f\"{passage_arr[0]}\\n{passage_arr[1]}\" for passage_arr in all_passages]\n",
    "    corpus_tokens = bm25s.tokenize(all_passages_text, stopwords=\"en\", stemmer=stemmer)\n",
    "    retriever = bm25s.BM25()\n",
    "    retriever.index(corpus_tokens)\n",
    "    query_tokens = bm25s.tokenize(query, stemmer=stemmer)\n",
    "    results, _ = retriever.retrieve(query_tokens, corpus=all_passages_text, k=top_k)\n",
    "    results = results.squeeze(0)\n",
    "    return {f\"top_{top_k}_passages\": results}  \n",
    "try:\n",
    "    new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "except:\n",
    "    print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "    create = True\n",
    "\n",
    "if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "    new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "    new_dataset = new_dataset.map(retrieve_top_passages, num_proc=num_proc)\n",
    "    new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "else:\n",
    "    print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "print(json.dumps(new_dataset['test'][0][f\"top_{top_k}_passages\"][0], indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the BM25 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4813a758d6ae422abfe2d4ee7047759f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/61.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ec2282359b4143adc28b098dd1f702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/139M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96be89572e043d8a6806aefbb3be705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/150M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6937c7795a6b4e64a4551c9d398292cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/161M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6b27ec89ee4ed39536b0149e108f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read file '/srv/elkhyo/.cache/huggingface/datasets/downloads/9f8282b6606f97fc6e9bcb72ff51694c8356b7d02e558355659f96253964cbbe' with error <class 'datasets.table.CastError'>: Couldn't cast\n",
      "docid: int64\n",
      "previous_text: string\n",
      "gold_text: string\n",
      "citations: list<element: list<element: string>>\n",
      "  child 0, element: list<element: string>\n",
      "      child 0, element: string\n",
      "oracle_documents_passages: list<element: list<element: string>>\n",
      "  child 0, element: list<element: string>\n",
      "      child 0, element: string\n",
      "oracle_documents_oracle_passages: list<element: list<element: string>>\n",
      "  child 0, element: list<element: string>\n",
      "      child 0, element: string\n",
      "top_10_passages: list<element: string>\n",
      "  child 0, element: string\n",
      "-- schema metadata --\n",
      "huggingface: '{\"info\": {\"features\": {\"docid\": {\"dtype\": \"int64\", \"_type\":' + 611\n",
      "to\n",
      "{'docid': Value(dtype='int64', id=None), 'previous_text': Value(dtype='string', id=None), 'gold_text': Value(dtype='string', id=None), 'gold_text_no_citations': Value(dtype='string', id=None), 'citations': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'oracle_documents_passages': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'oracle_documents_oracle_passages': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'top_10_passages': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n",
      "because column names don't match\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] ECHR_QA-generation-workshop not found in bm25_relevant_passages_oracle_documents\n",
      "[*] adding top_10_passages to ECHR_QA-generation-workshop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbd0826d1a84be58b2e59106133b1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=45):   0%|          | 0/1116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071c82946b524b24af6903f06244aea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0d0e51768c4defb583abd4893dccd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acac35b00c6c4a40afd3baf3cf6edbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"001-45580\\n79. The Commission finds that the measures of which the applicants complained constituted interferences with their right to respect for private and family life and the home, ensured by Article 8 para. 1 (Art. 8-1) of the Convention. It remains to be examined whether the interferences were justified under paragraph 2 of that Article or, in other words, whether they were \\\"in accordance with the law\\\", whether they pursued one or more legitimate aims under that paragraph, and whether they could be considered necessary in a democratic society for that or those aims.\"\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "data_dir = \"bm25_relevant_passages_oracle_documents\"\n",
    "\n",
    "create = False\n",
    "def retrieve_top_passages(entry):\n",
    "    query = entry['previous_text']\n",
    "    all_passages = entry['oracle_documents_passages']\n",
    "    all_passages_text = [f\"{passage_arr[0]}\\n{passage_arr[1]}\" for passage_arr in all_passages]\n",
    "    corpus_tokens = bm25s.tokenize(all_passages_text, stopwords=\"en\", stemmer=stemmer)\n",
    "    retriever = bm25s.BM25()\n",
    "    retriever.index(corpus_tokens)\n",
    "    query_tokens = bm25s.tokenize(query, stemmer=stemmer)\n",
    "    results, _ = retriever.retrieve(query_tokens, corpus=all_passages_text, k=top_k)\n",
    "    results = results.squeeze(0)\n",
    "    return {f\"top_{top_k}_passages\": results}  \n",
    "\n",
    "try:\n",
    "    new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "except:\n",
    "    print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "    create = True\n",
    "    \n",
    "if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "    new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "    new_dataset = new_dataset.map(retrieve_top_passages, num_proc=num_proc)\n",
    "    new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "else:\n",
    "    print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "print(json.dumps(new_dataset['test'][0][f\"top_{top_k}_passages\"][0], indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dense Oracle Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dense Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "encoder_name = \"jhu-clsp/LegalBERT-DPR-CLERC-ft\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n",
    "model = AutoModel.from_pretrained(encoder_name)\n",
    "# model = torch.compile(model)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "clean_encoder_name = encoder_name.replace(\"/\", \"_\")\n",
    "data_dir = \"dense_oracle_passages_oracle_documents\"\n",
    "data_dir = f\"{data_dir}/{clean_encoder_name}\"\n",
    "\n",
    "def normalize_embeddings(embeddings):\n",
    "    return embeddings / torch.norm(embeddings, p=2, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "def embed_texts(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=model.config.max_position_embeddings).to(device)\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        token_embeddings = model(**inputs).last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        embeddings = sum_embeddings / sum_mask\n",
    "    embeddings = normalize_embeddings(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def retrieve_top_passages_batch(entries):\n",
    "    all_queries = entries['gold_text']\n",
    "    all_passages_batch = entries['oracle_documents_passages']\n",
    "    all_passages_text = [passage[1] for passages in all_passages_batch for passage in passages]\n",
    "    query_embeddings = embed_texts(all_queries, tokenizer, model)  # Shape: (batch_size, dim)\n",
    "    passage_embeddings = embed_texts(all_passages_text, tokenizer, model)  # Shape: (total_passages, dim)\n",
    "    dim = passage_embeddings.size(1)\n",
    "    # index = faiss.IndexFlatL2(dim)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(passage_embeddings.cpu().numpy())  # FAISS requires NumPy arrays here\n",
    "    distances, indices = index.search(query_embeddings.cpu().numpy(), top_k)\n",
    "    top_passages_batch = [[all_passages_text[index] for index in index_arr] for index_arr in indices]\n",
    "    return {f\"top_{top_k}_passages\": top_passages_batch}\n",
    "\n",
    "try:\n",
    "    new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "except:\n",
    "    print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "    create = True\n",
    "    \n",
    "\n",
    "if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "    print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "    new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "    new_dataset = new_dataset.map(\n",
    "        lambda batch: retrieve_top_passages_batch(batch),\n",
    "        batched=True,\n",
    "        batch_size=4\n",
    "    )\n",
    "    new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "else:\n",
    "    print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "print(json.dumps(new_dataset['train'][0][f\"top_{top_k}_passages\"][0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_bm25_oracle_passages_oracle_documents = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"bm25_oracle_passages_oracle_documents\")\n",
    "print(\"===================================\")\n",
    "print(dataset_bm25_oracle_passages_oracle_documents['train'][0]['gold_text'])\n",
    "print(dataset_bm25_oracle_passages_oracle_documents['train'][0]['top_10_passages'][0])\n",
    "\n",
    "dataset_bm25_relevant_passages_oracle_documents = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"bm25_relevant_passages_oracle_documents\")\n",
    "print(\"===================================\")\n",
    "print(dataset_bm25_relevant_passages_oracle_documents['train'][0]['previous_text'])\n",
    "print(dataset_bm25_relevant_passages_oracle_documents['train'][0]['top_10_passages'][0])\n",
    "\n",
    "dataset_dense_oracle_passages_oracle_documents = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=\"dense_oracle_passages_oracle_documents\")\n",
    "print(\"===================================\")\n",
    "print(dataset_dense_oracle_passages_oracle_documents['train'][0]['gold_text'])\n",
    "print(dataset_dense_oracle_passages_oracle_documents['train'][0]['top_10_passages'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# import faiss\n",
    "# import numpy as np\n",
    "\n",
    "# encoder_name = \"jhu-clsp/LegalBERT-DPR-CLERC-ft\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n",
    "# model = AutoModel.from_pretrained(encoder_name)\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# top_k = 10\n",
    "# data_dir = \"dense_oracle_passages_oracle_documents\"\n",
    "\n",
    "# def embed_passages(passages, tokenizer, model):\n",
    "#     inputs = tokenizer(passages, return_tensors=\"pt\", padding=True, truncation=True, max_length=model.config.max_position_embeddings).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         embeddings = model(**inputs).pooler_output\n",
    "#     return embeddings.cpu().numpy()\n",
    "\n",
    "# def retrieve_top_passages(entry):\n",
    "#     query = entry['gold_text']\n",
    "#     all_passages = entry['oracle_documents_passages']\n",
    "#     all_passages_text = [f\"{passage_arr[0]}\\n{passage_arr[1]}\" for passage_arr in all_passages]\n",
    "    \n",
    "#     passage_embeddings = embed_passages(all_passages_text, tokenizer, model)\n",
    "#     query_embedding = embed_passages([query], tokenizer, model).squeeze(0)\n",
    "\n",
    "#     dim = passage_embeddings.shape[1]\n",
    "#     index = faiss.IndexFlatL2(dim)\n",
    "#     index.add(passage_embeddings)\n",
    "    \n",
    "#     # Retrieve top-k similar passages\n",
    "#     distances, indices = index.search(np.expand_dims(query_embedding, axis=0), top_k)\n",
    "#     top_passages = [all_passages_text[idx] for idx in indices[0]]\n",
    "    \n",
    "#     return {f\"top_{top_k}_passages\": top_passages}\n",
    "\n",
    "# try:\n",
    "#     workshop_hf_name = f\"CLERC-generation-workshop\"\n",
    "#     new_dataset = load_dataset(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "# except:\n",
    "#     print(f\"[!] {workshop_hf_name} not found in {data_dir}\")\n",
    "#     create = True\n",
    "    \n",
    "# if create or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['train'] or f\"top_{top_k}_passages\" not in current_chosen_dataset.column_names['test']:\n",
    "#     print(f\"[*] adding top_{top_k}_passages to {workshop_hf_name}\")\n",
    "#     new_dataset = DatasetDict({split: current_chosen_dataset[split] for split in current_chosen_dataset.keys()})\n",
    "#     new_dataset = new_dataset.map(retrieve_top_passages, num_proc=num_proc)\n",
    "#     new_dataset.push_to_hub(f\"ylkhayat/{workshop_hf_name}\", data_dir=data_dir)\n",
    "# else:\n",
    "#     print(f\"[!] top_{top_k}_passages already exists in {workshop_hf_name}\")\n",
    "# print(json.dumps(new_dataset['train'][0][f\"top_{top_k}_passages\"][0], indent=4)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
